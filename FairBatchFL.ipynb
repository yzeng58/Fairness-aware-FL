{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, os, copy, time\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from load_adult import *\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(option, logits, targets, distance, sensitive, mean_sensitive, larg = 1):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    fair_loss = torch.mul(sensitive - sensitive.type(torch.FloatTensor).mean(), distance.T[0])\n",
    "    fair_loss = torch.mean(torch.mul(fair_loss, fair_loss)) # modified mean to sum\n",
    "    if option == 'unconstrained':\n",
    "        return acc_loss, acc_loss, larg*fair_loss\n",
    "    if option == 'Zafar':\n",
    "        return acc_loss + larg*fair_loss, acc_loss, larg*fair_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import torch\n",
    "\n",
    "    \n",
    "    \n",
    "class FairBatch(Sampler):\n",
    "    \"\"\"FairBatch (Sampler in DataLoader).\n",
    "    \n",
    "    This class is for implementing the lambda adjustment and batch selection of FairBatch.\n",
    "\n",
    "    Attributes:\n",
    "        model: A model containing the intermediate states of the training.\n",
    "        x_, y_, z_data: Tensor-based train data.\n",
    "        alpha: A positive number for step size that used in the lambda adjustment.\n",
    "        fairness_type: A string indicating the target fairness type \n",
    "                       among original, demographic parity (dp), equal opportunity (eqopp), and equalized odds (eqodds).\n",
    "        replacement: A boolean indicating whether a batch consists of data with or without replacement.\n",
    "        N: An integer counting the size of data.\n",
    "        batch_size: An integer for the size of a batch.\n",
    "        batch_num: An integer for total number of batches in an epoch.\n",
    "        y_, z_item: Lists that contains the unique values of the y_data and z_data, respectively.\n",
    "        yz_tuple: Lists for pairs of y_item and z_item.\n",
    "        y_, z_, yz_mask: Dictionaries utilizing as array masks.\n",
    "        y_, z_, yz_index: Dictionaries containing the index of each class.\n",
    "        y_, z_, yz_len: Dictionaries containing the length information.\n",
    "        S: A dictionary containing the default size of each class in a batch.\n",
    "        lb1, lb2: (0~1) real numbers indicating the lambda values in FairBatch.\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, model, x_tensor, y_tensor, z_tensor, batch_size, alpha, target_fairness, replacement = False, seed = 0):\n",
    "        \"\"\"Initializes FairBatch.\"\"\"\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        self.x_data = x_tensor\n",
    "        self.y_data = y_tensor\n",
    "        self.z_data = z_tensor\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.fairness_type = target_fairness\n",
    "        self.replacement = replacement\n",
    "        \n",
    "        self.N = len(z_tensor)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.batch_num = int(len(self.y_data) / self.batch_size)\n",
    "        \n",
    "        # Takes the unique values of the tensors\n",
    "        self.z_item = list(set(z_tensor.tolist()))\n",
    "        self.y_item = list(set(y_tensor.tolist()))\n",
    "        \n",
    "        self.yz_tuple = list(itertools.product(self.y_item, self.z_item))\n",
    "        \n",
    "        # Makes masks\n",
    "        self.z_mask = {}\n",
    "        self.y_mask = {}\n",
    "        self.yz_mask = {}\n",
    "        \n",
    "        for tmp_z in self.z_item:\n",
    "            self.z_mask[tmp_z] = (self.z_data == tmp_z)\n",
    "            \n",
    "        for tmp_y in self.y_item:\n",
    "            self.y_mask[tmp_y] = (self.y_data == tmp_y)\n",
    "            \n",
    "        for tmp_yz in self.yz_tuple:\n",
    "            self.yz_mask[tmp_yz] = (self.y_data == tmp_yz[0]) & (self.z_data == tmp_yz[1])\n",
    "        \n",
    "\n",
    "        # Finds the index\n",
    "        self.z_index = {}\n",
    "        self.y_index = {}\n",
    "        self.yz_index = {}\n",
    "        \n",
    "        for tmp_z in self.z_item:\n",
    "            self.z_index[tmp_z] = (self.z_mask[tmp_z] == 1).nonzero().squeeze()\n",
    "            \n",
    "        for tmp_y in self.y_item:\n",
    "            self.y_index[tmp_y] = (self.y_mask[tmp_y] == 1).nonzero().squeeze()\n",
    "        \n",
    "        for tmp_yz in self.yz_tuple:\n",
    "            self.yz_index[tmp_yz] = (self.yz_mask[tmp_yz] == 1).nonzero().squeeze()\n",
    "            \n",
    "        # Length information\n",
    "        self.z_len = {}\n",
    "        self.y_len = {}\n",
    "        self.yz_len = {}\n",
    "        \n",
    "        for tmp_z in self.z_item:\n",
    "            self.z_len[tmp_z] = len(self.z_index[tmp_z])\n",
    "            \n",
    "        for tmp_y in self.y_item:\n",
    "            self.y_len[tmp_y] = len(self.y_index[tmp_y])\n",
    "            \n",
    "        for tmp_yz in self.yz_tuple:\n",
    "            self.yz_len[tmp_yz] = len(self.yz_index[tmp_yz])\n",
    "\n",
    "        # Default batch size\n",
    "        self.S = {}\n",
    "        \n",
    "        for tmp_yz in self.yz_tuple:\n",
    "            self.S[tmp_yz] = self.batch_size * (self.yz_len[tmp_yz])/self.N\n",
    "\n",
    "        \n",
    "        self.lb1 = (self.S[1,1])/(self.S[1,1]+(self.S[1,0]))\n",
    "        self.lb2 = (self.S[-1,1])/(self.S[-1,1]+(self.S[-1,0]))\n",
    "    \n",
    "    \n",
    "    def adjust_lambda(self):\n",
    "        \"\"\"Adjusts the lambda values for FairBatch algorithm.\n",
    "        \n",
    "        The detailed algorithms are decribed in the paper.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.model.eval()\n",
    "        logit = self.model(self.x_data)\n",
    "\n",
    "        criterion = torch.nn.BCELoss(reduction = 'none')\n",
    "        \n",
    "                \n",
    "        if self.fairness_type == 'eqopp':\n",
    "            \n",
    "            yhat_yz = {}\n",
    "            yhat_y = {}\n",
    "                        \n",
    "            eo_loss = criterion ((F.tanh(logit)+1)/2, (self.y_data+1)/2)\n",
    "            \n",
    "            for tmp_yz in self.yz_tuple:\n",
    "                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.yz_index[tmp_yz]])) / self.yz_len[tmp_yz]\n",
    "                \n",
    "            for tmp_y in self.y_item:\n",
    "                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.y_index[tmp_y]])) / self.y_len[tmp_y]\n",
    "            \n",
    "            # lb1 * loss_z1 + (1-lb1) * loss_z0\n",
    "            \n",
    "            if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n",
    "                self.lb1 += self.alpha\n",
    "            else:\n",
    "                self.lb1 -= self.alpha\n",
    "                \n",
    "            if self.lb1 < 0:\n",
    "                self.lb1 = 0\n",
    "            elif self.lb1 > 1:\n",
    "                self.lb1 = 1 \n",
    "                \n",
    "        elif self.fairness_type == 'eqodds':\n",
    "            \n",
    "            yhat_yz = {}\n",
    "            yhat_y = {}\n",
    "                        \n",
    "            eo_loss = criterion ((F.tanh(logit)+1)/2, (self.y_data+1)/2)\n",
    "            \n",
    "            for tmp_yz in self.yz_tuple:\n",
    "                yhat_yz[tmp_yz] = float(torch.sum(eo_loss[self.yz_index[tmp_yz]])) / self.yz_len[tmp_yz]\n",
    "                \n",
    "            for tmp_y in self.y_item:\n",
    "                yhat_y[tmp_y] = float(torch.sum(eo_loss[self.y_index[tmp_y]])) / self.y_len[tmp_y]\n",
    "            \n",
    "            y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n",
    "            y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n",
    "            \n",
    "            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n",
    "            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n",
    "            \n",
    "            if y1_diff > y0_diff:\n",
    "                if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n",
    "                    self.lb1 += self.alpha\n",
    "                else:\n",
    "                    self.lb1 -= self.alpha\n",
    "            else:\n",
    "                if yhat_yz[(-1, 1)] > yhat_yz[(-1, 0)]:\n",
    "                    self.lb2 += self.alpha\n",
    "                else:\n",
    "                    self.lb2 -= self.alpha\n",
    "                    \n",
    "                \n",
    "            if self.lb1 < 0:\n",
    "                self.lb1 = 0\n",
    "            elif self.lb1 > 1:\n",
    "                self.lb1 = 1\n",
    "                \n",
    "            if self.lb2 < 0:\n",
    "                self.lb2 = 0\n",
    "            elif self.lb2 > 1:\n",
    "                self.lb2 = 1\n",
    "                \n",
    "        elif self.fairness_type == 'dp':\n",
    "            yhat_yz = {}\n",
    "            yhat_y = {}\n",
    "            \n",
    "            ones_array = np.ones(len(self.y_data))\n",
    "            ones_tensor = torch.FloatTensor(ones_array)\n",
    "            dp_loss = criterion((F.tanh(logit)+1)/2, ones_tensor) # Note that ones tensor puts as the true label\n",
    "            \n",
    "            for tmp_yz in self.yz_tuple:\n",
    "                yhat_yz[tmp_yz] = float(torch.sum(dp_loss[self.yz_index[tmp_yz]])) / self.z_len[tmp_yz[1]]\n",
    "                    \n",
    "            \n",
    "            y1_diff = abs(yhat_yz[(1, 1)] - yhat_yz[(1, 0)])\n",
    "            y0_diff = abs(yhat_yz[(-1, 1)] - yhat_yz[(-1, 0)])\n",
    "            \n",
    "            # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n",
    "            # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n",
    "            \n",
    "            if y1_diff > y0_diff:\n",
    "                if yhat_yz[(1, 1)] > yhat_yz[(1, 0)]:\n",
    "                    self.lb1 += self.alpha\n",
    "                else:\n",
    "                    self.lb1 -= self.alpha\n",
    "            else:\n",
    "                if yhat_yz[(-1, 1)] > yhat_yz[(-1, 0)]: \n",
    "                    self.lb2 -= self.alpha\n",
    "                else:\n",
    "                    self.lb2 += self.alpha\n",
    "                    \n",
    "            if self.lb1 < 0:\n",
    "                self.lb1 = 0\n",
    "            elif self.lb1 > 1:\n",
    "                self.lb1 = 1\n",
    "                \n",
    "            if self.lb2 < 0:\n",
    "                self.lb2 = 0\n",
    "            elif self.lb2 > 1:\n",
    "                self.lb2 = 1\n",
    "\n",
    "\n",
    "    \n",
    "    def select_batch_replacement(self, batch_size, full_index, batch_num, replacement = False):\n",
    "        \"\"\"Selects a certain number of batches based on the given batch size.\n",
    "        \n",
    "        Args: \n",
    "            batch_size: An integer for the data size in a batch.\n",
    "            full_index: An array containing the candidate data indices.\n",
    "            batch_num: An integer indicating the number of batches.\n",
    "            replacement: A boolean indicating whether a batch consists of data with or without replacement.\n",
    "        \n",
    "        Returns:\n",
    "            Indices that indicate the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        select_index = []\n",
    "        \n",
    "        if replacement == True:\n",
    "            for _ in range(batch_num):\n",
    "                select_index.append(np.random.choice(full_index, batch_size, replace = False))\n",
    "        else:\n",
    "            tmp_index = full_index.detach().cpu().numpy().copy()\n",
    "            random.shuffle(tmp_index)\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i in range(batch_num):\n",
    "                if start_idx + batch_size > len(full_index):\n",
    "                    select_index.append(np.concatenate((tmp_index[start_idx:], tmp_index[ : batch_size - (len(full_index)-start_idx)])))\n",
    "                    \n",
    "                    start_idx = len(full_index)-start_idx\n",
    "                else:\n",
    "\n",
    "                    select_index.append(tmp_index[start_idx:start_idx + batch_size])\n",
    "                    start_idx += batch_size\n",
    "            \n",
    "        return select_index\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iters the full process of FairBatch for serving the batches to training.\n",
    "        \n",
    "        Returns:\n",
    "            Indices that indicate the data in each batch.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        if self.fairness_type == 'original':\n",
    "            \n",
    "            entire_index = torch.FloatTensor([i for i in range(len(self.y_data))])\n",
    "            \n",
    "            sort_index = self.select_batch_replacement(self.batch_size, entire_index, self.batch_num, self.replacement)\n",
    "            \n",
    "            for i in range(self.batch_num):\n",
    "                yield sort_index[i]\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            self.adjust_lambda() # Adjust the lambda values\n",
    "            each_size = {}\n",
    "            \n",
    "            \n",
    "            # Based on the updated lambdas, determine the size of each class in a batch\n",
    "            if self.fairness_type == 'eqopp':\n",
    "                # lb1 * loss_z1 + (1-lb1) * loss_z0\n",
    "                \n",
    "                each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(-1,1)] = round(self.S[(-1,1)])\n",
    "                each_size[(-1,0)] = round(self.S[(-1,0)])\n",
    "                \n",
    "            elif self.fairness_type == 'eqodds':\n",
    "                # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n",
    "                # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n",
    "\n",
    "                each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(-1,1)] = round(self.lb2 * (self.S[(-1,1)] + self.S[(-1,0)]))\n",
    "                each_size[(-1,0)] = round((1-self.lb2) * (self.S[(-1,1)] + self.S[(-1,0)]))\n",
    "                \n",
    "            elif self.fairness_type == 'dp':\n",
    "                # lb1 * loss_y1z1 + (1-lb1) * loss_y1z0\n",
    "                # lb2 * loss_y0z1 + (1-lb2) * loss_y0z0\n",
    "\n",
    "                each_size[(1,1)] = round(self.lb1 * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(1,0)] = round((1-self.lb1) * (self.S[(1,1)] + self.S[(1,0)]))\n",
    "                each_size[(-1,1)] = round(self.lb2 * (self.S[(-1,1)] + self.S[(-1,0)]))\n",
    "                each_size[(-1,0)] = round((1-self.lb2) * (self.S[(-1,1)] + self.S[(-1,0)]))\n",
    "\n",
    "\n",
    "            # Get the indices for each class\n",
    "            sort_index_y_1_z_1 = self.select_batch_replacement(each_size[(1, 1)], self.yz_index[(1,1)], self.batch_num, self.replacement)\n",
    "            sort_index_y_0_z_1 = self.select_batch_replacement(each_size[(-1, 1)], self.yz_index[(-1,1)], self.batch_num, self.replacement)\n",
    "            sort_index_y_1_z_0 = self.select_batch_replacement(each_size[(1, 0)], self.yz_index[(1,0)], self.batch_num, self.replacement)\n",
    "            sort_index_y_0_z_0 = self.select_batch_replacement(each_size[(-1, 0)], self.yz_index[(-1,0)], self.batch_num, self.replacement)\n",
    "            \n",
    "                \n",
    "            for i in range(self.batch_num):\n",
    "                key_in_fairbatch = sort_index_y_0_z_0[i].copy()\n",
    "                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_0[i].copy()))\n",
    "                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_0_z_1[i].copy()))\n",
    "                key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_1[i].copy()))\n",
    "                             \n",
    "                random.shuffle(key_in_fairbatch)\n",
    "\n",
    "                yield key_in_fairbatch\n",
    "                               \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of data.\"\"\"\n",
    "        \n",
    "        return len(self.y_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "    def __init__(self, dataset, idxs, batch_size, option, sampler = None, penalty = 0):\n",
    "        self.trainloader, self.validloader = self.train_val(dataset, list(idxs), batch_size, sampler)\n",
    "        self.option = option\n",
    "        self.penalty = penalty\n",
    "            \n",
    "    def train_val(self, dataset, idxs, batch_size, sampler = None):\n",
    "        \"\"\"\n",
    "        Returns train, validation for a given local training dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # split indexes for train, validation (90, 10)\n",
    "        idxs_train = idxs[:int(0.9*len(idxs))]\n",
    "        idxs_val = idxs[int(0.9*len(idxs)):len(idxs)]\n",
    "        \n",
    "        if sampler: # FairBatch\n",
    "            trainloader = DataLoader(DatasetSplit(dataset, idxs_train), sampler = sampler,\n",
    "                                     batch_size=batch_size, shuffle=True)\n",
    "            validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                                     batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "            return trainloader, validloader\n",
    "            \n",
    "        else:\n",
    "            trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                                     batch_size=batch_size, shuffle=True)\n",
    "            validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                                     batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "            return trainloader, validloader\n",
    "\n",
    "    def update_weights(self, model, global_round, learning_rate, local_epochs, optimizer):\n",
    "        # Set mode to train model\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "\n",
    "        # Set optimizer for the local updates\n",
    "        if optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                                        ) # momentum=0.5\n",
    "        elif optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                         weight_decay=1e-4)\n",
    "\n",
    "        for i in range(local_epochs):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (features, labels, sensitive) in enumerate(self.trainloader):\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE).type(torch.LongTensor)\n",
    "                # we need to set the gradients to zero before starting to do backpropragation \n",
    "                # because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "                # This is convenient while training RNNs\n",
    "                \n",
    "                log_probs, logits = model(features)\n",
    "                loss, _, _ = loss_func(self.option,\n",
    "                    logits, labels, logits, sensitive, mean_sensitive, self.penalty)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'.format(\n",
    "                        global_round, i, batch_idx * len(features),\n",
    "                        len(self.trainloader.dataset),\n",
    "                        100. * batch_idx / len(self.trainloader), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "        # weight, loss\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "    def inference(self, model):\n",
    "        \"\"\" \n",
    "        Returns the inference accuracy, \n",
    "                                loss, \n",
    "                                N(sensitive group, pos), \n",
    "                                N(non-sensitive group, pos), \n",
    "                                N(sensitive group),\n",
    "                                N(non-sensitive group),\n",
    "                                acc_loss,\n",
    "                                fair_loss\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "        loss, total, correct, fair_loss, acc_loss, num_batch = 0.0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "        sp, nsp, s, n = 0, 0, 0, 0\n",
    "        for batch_idx, (features, labels, sensitive) in enumerate(self.validloader):\n",
    "            features, labels = features.to(DEVICE), labels.to(DEVICE).type(torch.LongTensor)\n",
    "            sensitive = sensitive.to(DEVICE)\n",
    "            \n",
    "            # Inference\n",
    "            outputs, logits = model(features)\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            bool_correct = torch.eq(pred_labels, labels)\n",
    "            correct += torch.sum(bool_correct).item()\n",
    "            total += len(labels)\n",
    "            num_batch += 1\n",
    "            bool_sensitive = torch.eq(sensitive, torch.ones(len(labels)))\n",
    "            s += torch.sum(bool_sensitive).item()\n",
    "            n += torch.sum(torch.logical_not(bool_sensitive)).item()\n",
    "            sp += torch.sum(torch.logical_and(pred_labels, bool_sensitive)).item()\n",
    "            nsp += torch.sum(torch.logical_and(pred_labels, torch.logical_not(bool_sensitive))).item()\n",
    "            \n",
    "            batch_loss, batch_acc_loss, batch_fair_loss = loss_func(self.option, outputs, \n",
    "                                                        labels, logits, sensitive, mean_sensitive, self.penalty)\n",
    "            loss, acc_loss, fair_loss = (loss + batch_loss.item(), \n",
    "                                         acc_loss + batch_acc_loss.item(), \n",
    "                                         fair_loss + batch_fair_loss.item())\n",
    "        accuracy = correct/total\n",
    "        return accuracy, loss, s, n, sp, nsp, acc_loss / num_batch, fair_loss / num_batch\n",
    "\n",
    "\n",
    "def test_inference(model, test_dataset, batch_size):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    sp, nsp, s, n = 0, 0, 0, 0\n",
    "    \n",
    "    criterion = nn.NLLLoss().to(DEVICE)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "    for batch_idx, (features, labels, sensitive) in enumerate(testloader):\n",
    "        features = features.to(DEVICE)\n",
    "        labels =  labels.to(DEVICE).type(torch.LongTensor)\n",
    "        # Inference\n",
    "        outputs, logits = model(features)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        bool_correct = torch.eq(pred_labels, labels)\n",
    "        correct += torch.sum(bool_correct).item()\n",
    "        total += len(labels)\n",
    "\n",
    "        bool_sensitive = torch.eq(sensitive, torch.ones(len(labels)))\n",
    "        s += torch.sum(bool_sensitive).item()\n",
    "        n += torch.sum(torch.logical_not(bool_sensitive) ).item()\n",
    "        sp += torch.sum(torch.logical_and(bool_correct, bool_sensitive)).item()\n",
    "        nsp += torch.sum(torch.logical_and(bool_correct, torch.logical_not(bool_sensitive))).item()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    # |P(Group1, pos) - P(Group2, pos)| = |N(Group1, pos)/N(Group1) - N(Group2, pos)/N(Group2)|\n",
    "    return accuracy, loss, abs(sp/s-nsp/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, option = \"unconstrained\", batch_size = 128, num_clients = 2\n",
    "          num_rounds = 5, learning_rate = 0.01, optimizer = 'adam', local_epochs= 5, \n",
    "          num_workers = 4, print_every = 1,\n",
    "         penalty = 1):\n",
    "    \"\"\"\n",
    "    Server execution.\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    start_time = time.time()\n",
    "    weights = model.state_dict()\n",
    "    \n",
    "    test_loader = DataLoader(dataset = test_dataset,\n",
    "                            batch_size = batch_size,\n",
    "                            num_workers = num_workers)\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_dataset,\n",
    "                        batch_size = batch_size,\n",
    "                        num_workers = num_workers)\n",
    "\n",
    "    def average_weights(w):\n",
    "        \"\"\"\n",
    "        Returns the average of the weights.\n",
    "        \"\"\"\n",
    "        w_avg = copy.deepcopy(w[0])\n",
    "        for key in w_avg.keys():\n",
    "            for i in range(1, len(w)):\n",
    "                w_avg[key] += w[i][key]\n",
    "            w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "        return w_avg\n",
    "\n",
    "    if option == 'FairBatch':\n",
    "        # initialize the lambda\n",
    "        sampler = [((adult.salary == 0) & (adult[sen_var] == 0)).mean(), \n",
    "               ((adult.salary == 0) & (adult[sen_var] == 1)).mean()]\n",
    "    else:\n",
    "        sampler = None\n",
    "    \n",
    "    for round_ in tqdm(range(num_rounds)):\n",
    "        local_weights, local_losses = [], []\n",
    "        print(f'\\n | Global Training Round : {round_+1} |\\n')\n",
    "\n",
    "        model.train()\n",
    "        m = 2 # the number of clients to be chosen in each round_\n",
    "        idxs_users = np.random.choice(range(num_clients), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = ClientUpdate(dataset=train_dataset,\n",
    "                                        idxs=clients_idx[idx], batch_size = batch_size, \n",
    "                                       option = option, penalty = penalty, sampler = sampler)\n",
    "            w, loss = local_model.update_weights(\n",
    "                            model=copy.deepcopy(model), global_round=round_, \n",
    "                                learning_rate = learning_rate, local_epochs = local_epochs, \n",
    "                                optimizer = optimizer)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        # update global weights\n",
    "        weights = average_weights(local_weights)\n",
    "        model.load_state_dict(weights)\n",
    "        \n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # Calculate avg training accuracy over all users at every round\n",
    "        list_acc, list_loss = [], []\n",
    "        s, n, sp, nsp = 0, 0, 0, 0\n",
    "        model.eval()\n",
    "        for c in range(m):\n",
    "            local_model = ClientUpdate(dataset=train_dataset,\n",
    "                                        idxs=clients_idx[c], batch_size = batch_size, option = option, \n",
    "                                       penalty = penalty, sampler = sampler)\n",
    "            # validation dataset inference\n",
    "            acc, loss, s_, n_, sp_, nsp_, acc_loss, fair_loss = local_model.inference(model=model) \n",
    "            list_acc.append(acc)\n",
    "            list_loss.append(loss)\n",
    "            s, n, sp, nsp = s + s_, n + n_, sp + sp_, nsp + nsp_\n",
    "            print(\"Client %d: accuracy loss: %.2f | fairness loss %.2f | RD = %.2f = |%d/%d-%d/%d| \" % (\n",
    "                c, acc_loss, fair_loss, abs(sp_/s_-nsp_/n_), sp_, s_, nsp_, n_))\n",
    "            \n",
    "        train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (round_+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {round_+1} global rounds:')\n",
    "            print(\"Training loss: %.2f | Validation accuracy: %.2f%% | Validation RD: %.2f\" % (\n",
    "                 np.mean(np.array(train_loss)), \n",
    "                100*train_accuracy[-1],\n",
    "                abs(sp/s-nsp/n)\n",
    "                 ))\n",
    "\n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss, rd= test_inference(model, test_dataset, batch_size)\n",
    "\n",
    "    print(f' \\n Results after {num_rounds} global rounds of training:')\n",
    "    print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "    print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "    # Compute RD: risk difference - fairness metric\n",
    "    # |P(Group1, pos) - P(Group2, pos)| = |N(Group1, pos)/N(Group1) - N(Group2, pos)/N(Group2)|\n",
    "    print(\"|---- Test RD: {:.2f}\".format(rd))\n",
    "\n",
    "    print('\\n Total Run Time: {0:0.4f} sec'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 97.129074\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 88.739372\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 58.027599\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 73.519951\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 63.784790\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 75.687607\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 51.680065\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 74.850227\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 60.017704\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 75.023788\n",
      "| Global Round : 0 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 40.387955\n",
      "| Global Round : 0 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 50.170868\n",
      "| Global Round : 0 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 53.864670\n",
      "| Global Round : 0 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 70.707001\n",
      "| Global Round : 0 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 77.145119\n",
      "| Global Round : 0 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 87.636597\n",
      "| Global Round : 0 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 48.997784\n",
      "| Global Round : 0 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 107.260017\n",
      "| Global Round : 0 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 63.912033\n",
      "| Global Round : 0 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 114.068871\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 98.088036\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 95.289825\n",
      "| Global Round : 0 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 77.870079\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 48.414394\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 159.764465\n",
      "| Global Round : 0 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 57.173935\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 30.913012\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 24.362934\n",
      "| Global Round : 0 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 84.380501\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 46.284508\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 64.378204\n",
      "| Global Round : 0 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 94.755600\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 48.771263\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 60.414413\n",
      "| Global Round : 0 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 143.476501\n",
      "| Global Round : 0 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 34.604496\n",
      "| Global Round : 0 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 55.154663\n",
      "| Global Round : 0 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 89.567863\n",
      "| Global Round : 0 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 53.202686\n",
      "| Global Round : 0 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 52.690334\n",
      "| Global Round : 0 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 55.480968\n",
      "| Global Round : 0 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 37.404533\n",
      "| Global Round : 0 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 62.226818\n",
      "| Global Round : 0 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 88.869049\n",
      "| Global Round : 0 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 42.501511\n",
      "| Global Round : 0 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 56.758282\n",
      "| Global Round : 0 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 49.223980\n",
      "| Global Round : 0 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 35.380726\n",
      "| Global Round : 0 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 55.768646\n",
      "| Global Round : 0 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 39.540443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:07<00:31,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 115.47 | fairness loss 5.94 | RD = 0.38 = |140/592-877/1421| \n",
      "Client 1: accuracy loss: 69.04 | fairness loss 7.19 | RD = 0.30 = |88/421-423/823| \n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: 81.70 | Validation accuracy: 75.11% | Validation RD: 0.35\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 74.821060\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 68.413315\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 52.999275\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 38.131889\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 56.216457\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 50.668480\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 45.660702\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 52.844872\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 55.896729\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 78.781776\n",
      "| Global Round : 1 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 51.799438\n",
      "| Global Round : 1 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 44.609287\n",
      "| Global Round : 1 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 53.064373\n",
      "| Global Round : 1 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 58.495209\n",
      "| Global Round : 1 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 45.167358\n",
      "| Global Round : 1 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 50.997551\n",
      "| Global Round : 1 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 56.772511\n",
      "| Global Round : 1 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 57.771523\n",
      "| Global Round : 1 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 45.333839\n",
      "| Global Round : 1 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 85.112267\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 35.536892\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 141.072540\n",
      "| Global Round : 1 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 91.145462\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 39.044327\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 95.434227\n",
      "| Global Round : 1 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 89.685760\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 54.009243\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 93.486496\n",
      "| Global Round : 1 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 44.332840\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 48.056149\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 70.521973\n",
      "| Global Round : 1 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 68.614624\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 35.320141\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 100.817520\n",
      "| Global Round : 1 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 71.157928\n",
      "| Global Round : 1 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 56.523975\n",
      "| Global Round : 1 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 71.585754\n",
      "| Global Round : 1 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 75.531631\n",
      "| Global Round : 1 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 45.782578\n",
      "| Global Round : 1 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 66.575043\n",
      "| Global Round : 1 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 38.456276\n",
      "| Global Round : 1 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 39.081375\n",
      "| Global Round : 1 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 110.984131\n",
      "| Global Round : 1 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 79.280754\n",
      "| Global Round : 1 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 53.188866\n",
      "| Global Round : 1 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 53.482285\n",
      "| Global Round : 1 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 113.295044\n",
      "| Global Round : 1 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 56.419384\n",
      "| Global Round : 1 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 57.679600\n",
      "| Global Round : 1 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 82.219818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:16<00:24,  8.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 101.69 | fairness loss 3.12 | RD = 0.21 = |76/592-487/1421| \n",
      "Client 1: accuracy loss: 60.96 | fairness loss 4.47 | RD = 0.18 = |44/421-231/823| \n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training loss: 76.63 | Validation accuracy: 82.88% | Validation RD: 0.20\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 59.690285\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 72.011383\n",
      "| Global Round : 2 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 48.918304\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 52.151093\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 149.107239\n",
      "| Global Round : 2 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 98.614044\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 39.356712\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 146.139618\n",
      "| Global Round : 2 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 42.178570\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 37.914608\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 68.632233\n",
      "| Global Round : 2 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 105.329315\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 52.529430\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 43.857479\n",
      "| Global Round : 2 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 147.092667\n",
      "| Global Round : 2 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 45.074829\n",
      "| Global Round : 2 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 76.998764\n",
      "| Global Round : 2 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 49.109997\n",
      "| Global Round : 2 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 36.890823\n",
      "| Global Round : 2 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 75.179916\n",
      "| Global Round : 2 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 96.450363\n",
      "| Global Round : 2 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 47.650890\n",
      "| Global Round : 2 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 58.250805\n",
      "| Global Round : 2 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 110.648941\n",
      "| Global Round : 2 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 40.660206\n",
      "| Global Round : 2 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 55.180538\n",
      "| Global Round : 2 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 51.512520\n",
      "| Global Round : 2 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 66.353287\n",
      "| Global Round : 2 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 51.812714\n",
      "| Global Round : 2 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 70.045319\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 56.783043\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 53.826298\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 45.775803\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 58.359493\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 52.673756\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 92.093544\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 48.020672\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 52.119778\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 48.854614\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 61.902184\n",
      "| Global Round : 2 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 45.619896\n",
      "| Global Round : 2 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 118.712929\n",
      "| Global Round : 2 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 51.937500\n",
      "| Global Round : 2 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 43.593868\n",
      "| Global Round : 2 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 46.984524\n",
      "| Global Round : 2 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 77.558372\n",
      "| Global Round : 2 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 43.967041\n",
      "| Global Round : 2 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 72.770325\n",
      "| Global Round : 2 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 55.309082\n",
      "| Global Round : 2 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 51.951122\n",
      "Client 0: accuracy loss: 106.53 | fairness loss 0.28 | RD = 0.24 = |98/592-579/1421| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:25<00:16,  8.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: accuracy loss: 64.36 | fairness loss 0.33 | RD = 0.23 = |57/421-303/823| \n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training loss: 73.63 | Validation accuracy: 82.65% | Validation RD: 0.24\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 42.475834\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 67.931946\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 39.536667\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 69.762161\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 54.083927\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 39.197651\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 41.307663\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 55.150444\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 62.792068\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 73.305267\n",
      "| Global Round : 3 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 63.506817\n",
      "| Global Round : 3 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 57.705238\n",
      "| Global Round : 3 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 56.515770\n",
      "| Global Round : 3 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 42.933571\n",
      "| Global Round : 3 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 52.393600\n",
      "| Global Round : 3 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 55.708553\n",
      "| Global Round : 3 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 57.756763\n",
      "| Global Round : 3 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 42.749187\n",
      "| Global Round : 3 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 53.637402\n",
      "| Global Round : 3 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 68.624451\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 39.426472\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 39.495693\n",
      "| Global Round : 3 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 93.122559\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 49.462891\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 28.644646\n",
      "| Global Round : 3 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 55.423557\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 33.924416\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 80.226494\n",
      "| Global Round : 3 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 69.109299\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 32.380135\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 82.210190\n",
      "| Global Round : 3 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 72.091751\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 47.459568\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 43.404251\n",
      "| Global Round : 3 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 84.179390\n",
      "| Global Round : 3 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 47.204185\n",
      "| Global Round : 3 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 121.637634\n",
      "| Global Round : 3 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 52.777859\n",
      "| Global Round : 3 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 53.150143\n",
      "| Global Round : 3 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 48.094990\n",
      "| Global Round : 3 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 39.618126\n",
      "| Global Round : 3 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 39.322639\n",
      "| Global Round : 3 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 35.949448\n",
      "| Global Round : 3 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 60.561394\n",
      "| Global Round : 3 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 45.793541\n",
      "| Global Round : 3 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 103.603386\n",
      "| Global Round : 3 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 53.477566\n",
      "| Global Round : 3 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 53.699974\n",
      "| Global Round : 3 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 100.947594\n",
      "| Global Round : 3 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 84.237503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:33<00:08,  8.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 105.01 | fairness loss 0.25 | RD = 0.26 = |80/592-563/1421| \n",
      "Client 1: accuracy loss: 63.38 | fairness loss 0.24 | RD = 0.25 = |46/421-296/823| \n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training loss: 72.13 | Validation accuracy: 83.13% | Validation RD: 0.26\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 41.820972\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 58.578114\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 52.752613\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 65.227402\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 41.261173\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 54.598175\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 49.818287\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 87.625832\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 55.013706\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 44.190445\n",
      "| Global Round : 4 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 56.730701\n",
      "| Global Round : 4 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 44.391773\n",
      "| Global Round : 4 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 55.510426\n",
      "| Global Round : 4 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 66.437317\n",
      "| Global Round : 4 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 50.939125\n",
      "| Global Round : 4 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 59.893547\n",
      "| Global Round : 4 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 42.480869\n",
      "| Global Round : 4 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 64.873260\n",
      "| Global Round : 4 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 58.769352\n",
      "| Global Round : 4 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 51.928299\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 48.069996\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 47.488205\n",
      "| Global Round : 4 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 52.612099\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 97.347885\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 89.166000\n",
      "| Global Round : 4 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 73.205559\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 50.746319\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 57.851093\n",
      "| Global Round : 4 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 62.077255\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 43.189537\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 55.078159\n",
      "| Global Round : 4 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 122.506302\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 34.584358\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 87.425217\n",
      "| Global Round : 4 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 138.550720\n",
      "| Global Round : 4 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 52.219971\n",
      "| Global Round : 4 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 92.714012\n",
      "| Global Round : 4 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 102.162781\n",
      "| Global Round : 4 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 45.664753\n",
      "| Global Round : 4 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 92.902267\n",
      "| Global Round : 4 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 45.793530\n",
      "| Global Round : 4 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 49.597023\n",
      "| Global Round : 4 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 94.598595\n",
      "| Global Round : 4 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 45.609135\n",
      "| Global Round : 4 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 69.084282\n",
      "| Global Round : 4 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 58.455456\n",
      "| Global Round : 4 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 58.325356\n",
      "| Global Round : 4 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 34.397278\n",
      "| Global Round : 4 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 79.276871\n",
      "| Global Round : 4 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 56.993767\n",
      "Client 0: accuracy loss: 104.09 | fairness loss 1.00 | RD = 0.26 = |68/592-529/1421| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:40<00:00,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: accuracy loss: 62.64 | fairness loss 1.26 | RD = 0.23 = |42/421-274/823| \n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training loss: 71.22 | Validation accuracy: 83.88% | Validation RD: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 5 global rounds of training:\n",
      "|---- Avg Train Accuracy: 83.88%\n",
      "|---- Test Accuracy: 84.48%\n",
      "|---- Test RD: 0.12\n",
      "\n",
      " Total Run Time: 41.2722 sec\n"
     ]
    }
   ],
   "source": [
    "train(logReg(num_features=NUM_FEATURES, num_classes=2), \n",
    "      \"Zafar\", penalty = 50, optimizer = 'sgd', learning_rate = 0.01,\n",
    "     num_rounds = 5, local_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 93.002548\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 69.492393\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 47.031193\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 54.340210\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 57.740227\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 42.863007\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 35.554657\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 37.808662\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 40.819706\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 60.515236\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 93.193375\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 61.420052\n",
      "| Global Round : 0 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 36.336704\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 40.868534\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 52.041531\n",
      "| Global Round : 0 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 44.657166\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 36.367760\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 38.178757\n",
      "| Global Round : 0 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 39.489773\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 53.710529\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 39.745106\n",
      "| Global Round : 0 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 47.663551\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 38.713913\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 41.190144\n",
      "| Global Round : 0 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 45.539818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:03<00:15,  3.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 105.00 | fairness loss 0.37 | RD = 0.38 = |104/592-784/1421| \n",
      "Client 1: accuracy loss: 61.40 | fairness loss 0.52 | RD = 0.29 = |62/421-363/823| \n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: 48.38 | Validation accuracy: 78.70% | Validation RD: 0.35\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 27.846607\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 48.043636\n",
      "| Global Round : 1 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 61.422527\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 40.716099\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 47.060272\n",
      "| Global Round : 1 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 39.354794\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 49.975140\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 38.019928\n",
      "| Global Round : 1 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 31.296812\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 49.768524\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 41.729084\n",
      "| Global Round : 1 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 33.447220\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 37.975605\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 43.921574\n",
      "| Global Round : 1 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 48.328514\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 61.016708\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 43.808407\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 43.098030\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 46.950306\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 46.926586\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 33.482834\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 49.261879\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 52.704826\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 36.153240\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 55.809326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:08<00:11,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 102.19 | fairness loss 0.41 | RD = 0.27 = |97/592-615/1421| \n",
      "Client 1: accuracy loss: 60.48 | fairness loss 0.53 | RD = 0.25 = |53/421-307/823| \n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training loss: 47.05 | Validation accuracy: 81.91% | Validation RD: 0.26\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 50.165337\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 43.332649\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 49.328442\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 49.271786\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 39.606594\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 46.713963\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 44.118191\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 47.675804\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 46.641232\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 57.602432\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 40.648045\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 38.049118\n",
      "| Global Round : 2 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 40.316032\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 43.569111\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 41.866032\n",
      "| Global Round : 2 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 49.612507\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 50.965942\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 28.584726\n",
      "| Global Round : 2 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 51.980751\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 40.594128\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 45.237785\n",
      "| Global Round : 2 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 41.952728\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 43.510086\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 37.154301\n",
      "| Global Round : 2 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 49.350651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:12<00:08,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 101.28 | fairness loss 0.47 | RD = 0.29 = |86/592-622/1421| \n",
      "Client 1: accuracy loss: 60.17 | fairness loss 0.57 | RD = 0.28 = |47/421-323/823| \n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training loss: 46.47 | Validation accuracy: 81.86% | Validation RD: 0.29\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 37.018890\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 64.279480\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 59.291298\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 41.470345\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 45.804871\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 50.384151\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 44.137280\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 65.056488\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 46.620605\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 65.099709\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 41.579872\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 42.045959\n",
      "| Global Round : 3 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 36.269276\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 35.665485\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 36.050652\n",
      "| Global Round : 3 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 63.275856\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 50.722237\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 57.182693\n",
      "| Global Round : 3 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 31.293236\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 50.105518\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 36.557648\n",
      "| Global Round : 3 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 40.490582\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 39.740265\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 35.755054\n",
      "| Global Round : 3 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 45.686794\n",
      "Client 0: accuracy loss: 99.14 | fairness loss 0.52 | RD = 0.21 = |79/592-484/1421| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:16<00:04,  4.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: accuracy loss: 59.30 | fairness loss 0.61 | RD = 0.20 = |45/421-250/823| \n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training loss: 45.98 | Validation accuracy: 83.74% | Validation RD: 0.20\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 37.388119\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 57.172165\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 60.511341\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 54.603451\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 48.786533\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 40.720695\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 49.488594\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 50.179955\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 50.713879\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 50.901196\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 40.407482\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 34.762123\n",
      "| Global Round : 4 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 32.151657\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 46.663422\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 47.465000\n",
      "| Global Round : 4 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 38.424545\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 38.347801\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 47.645927\n",
      "| Global Round : 4 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 30.696125\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 32.082710\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 52.631165\n",
      "| Global Round : 4 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 30.177355\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 37.050884\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 30.635870\n",
      "| Global Round : 4 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 38.065609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:20<00:00,  4.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 98.95 | fairness loss 0.55 | RD = 0.24 = |76/592-521/1421| \n",
      "Client 1: accuracy loss: 59.18 | fairness loss 0.64 | RD = 0.24 = |44/421-283/823| \n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training loss: 45.62 | Validation accuracy: 83.51% | Validation RD: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 5 global rounds of training:\n",
      "|---- Avg Train Accuracy: 83.51%\n",
      "|---- Test Accuracy: 84.52%\n",
      "|---- Test RD: 0.12\n",
      "\n",
      " Total Run Time: 21.0496 sec\n"
     ]
    }
   ],
   "source": [
    "train(logReg(num_features=NUM_FEATURES, num_classes=2), optimizer = 'sgd', learning_rate = 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
