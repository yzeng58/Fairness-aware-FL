{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, copy, time, itertools, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from load_adult import *\n",
    "from utils import *\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(option, logits, targets, distance, sensitive, mean_sensitive, larg = 1):\n",
    "    acc_loss = F.cross_entropy(logits, targets, reduction = 'sum')\n",
    "    fair_loss = torch.mul(sensitive - sensitive.type(torch.FloatTensor).mean(), distance.T[0])\n",
    "    fair_loss = torch.mean(torch.mul(fair_loss, fair_loss)) # modified mean to sum\n",
    "    if option == 'unconstrained':\n",
    "        return acc_loss, acc_loss, larg*fair_loss\n",
    "    if option == 'Zafar':\n",
    "        return acc_loss + larg*fair_loss, acc_loss, larg*fair_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FairBatch(Sampler):\n",
    "    \"\"\"FairBatch (Sampler in DataLoader).\n",
    "    \n",
    "    This class is for implementing batch selection of FairBatch.\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, train_dataset, lbd, client_idx, batch_size, replacement = False, seed = 0):\n",
    "        \"\"\"Initializes FairBatch.\"\"\"\n",
    "        \n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.N = train_dataset.y.shape[0]\n",
    "        self.batch_num = int(self.N / self.batch_size)\n",
    "        self.lbd = lbd\n",
    "        \n",
    "        self.yz_index = {}\n",
    "        \n",
    "        for y, z in itertools.product([0,1], [0,1]):\n",
    "                self.yz_index[(y,z)] = np.where((train_dataset.y == y) & (train_dataset.sen == z))[0]\n",
    "\n",
    "    def select_batch_replacement(self, batch_size, full_index, batch_num, replacement = False):\n",
    "        \"\"\"Selects a certain number of batches based on the given batch size.\n",
    "        \n",
    "        Args: \n",
    "            batch_size: An integer for the data size in a batch.\n",
    "            full_index: An array containing the candidate data indices.\n",
    "            batch_num: An integer indicating the number of batches.\n",
    "            replacement: A boolean indicating whether a batch consists of data with or without replacement.\n",
    "        \n",
    "        Returns:\n",
    "            Indices that indicate the data.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        select_index = []\n",
    "        \n",
    "        if replacement == True:\n",
    "            for _ in range(batch_num):\n",
    "                select_index.append(np.random.choice(full_index, batch_size, replace = False))\n",
    "        else:\n",
    "            tmp_index = full_index.copy()\n",
    "            random.shuffle(tmp_index)\n",
    "            \n",
    "            start_idx = 0\n",
    "            for i in range(batch_num):\n",
    "                if start_idx + batch_size > len(full_index):\n",
    "                    select_index.append(np.concatenate((tmp_index[start_idx:], tmp_index[ : batch_size - (len(full_index)-start_idx)])))\n",
    "                    \n",
    "                    start_idx = len(full_index)-start_idx\n",
    "                else:\n",
    "\n",
    "                    select_index.append(tmp_index[start_idx:start_idx + batch_size])\n",
    "                    start_idx += batch_size\n",
    "            \n",
    "        return select_index\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iters the full process of FairBatch for serving the batches to training.\n",
    "        \n",
    "        Returns:\n",
    "            Indices that indicate the data in each batch.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        # Get the indices for each class\n",
    "        sort_index_y_1_z_1 = self.select_batch_replacement(int(self.lbd[(1,1)] * self.N), self.yz_index[(1,1)], self.batch_num)\n",
    "        sort_index_y_0_z_1 = self.select_batch_replacement(int(self.lbd[(0,1)] * self.N), self.yz_index[(0,1)], self.batch_num)\n",
    "        sort_index_y_1_z_0 = self.select_batch_replacement(int(self.lbd[(1,0)] * self.N), self.yz_index[(1,0)], self.batch_num)\n",
    "        sort_index_y_0_z_0 = self.select_batch_replacement(int(self.lbd[(0,0)] * self.N), self.yz_index[(0,0)], self.batch_num)\n",
    "\n",
    "\n",
    "        for i in range(self.batch_num):\n",
    "            key_in_fairbatch = sort_index_y_0_z_0[i].copy()\n",
    "            key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_0[i].copy()))\n",
    "            key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_0_z_1[i].copy()))\n",
    "            key_in_fairbatch = np.hstack((key_in_fairbatch, sort_index_y_1_z_1[i].copy()))\n",
    "\n",
    "            random.shuffle(key_in_fairbatch)\n",
    "            \n",
    "            print(key_in_fairbatch.shape)\n",
    "            yield key_in_fairbatch\n",
    "                               \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of data.\"\"\"\n",
    "        \n",
    "        return self.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClientUpdate(object):\n",
    "    def __init__(self, dataset, idxs, batch_size, option, penalty = 0, lbd = None):\n",
    "        self.trainloader, self.validloader = self.train_val(dataset, list(idxs), batch_size, option, lbd)\n",
    "        self.dataset = dataset\n",
    "        self.option = option\n",
    "        self.penalty = penalty\n",
    "            \n",
    "    def train_val(self, dataset, idxs, batch_size, option, lbd):\n",
    "        \"\"\"\n",
    "        Returns train, validation for a given local training dataset\n",
    "        and user indexes.\n",
    "        \"\"\"\n",
    "        \n",
    "        # split indexes for train, validation (90, 10)\n",
    "        idxs_train = idxs[:int(0.9*len(idxs))]\n",
    "        idxs_val = idxs[int(0.9*len(idxs)):len(idxs)]\n",
    "        \n",
    "        if option == \"FairBatch\": \n",
    "            # FairBatch(self, train_dataset, lbd, client_idx, batch_size, replacement = False, seed = 0)\n",
    "            sampler = FairBatch(DatasetSplit(dataset, idxs_train), lbd, idxs,\n",
    "                                 batch_size = batch_size, replacement = False, seed = 0)\n",
    "            trainloader = DataLoader(DatasetSplit(dataset, idxs_train), sampler = sampler,\n",
    "                                     batch_size=batch_size)\n",
    "                        \n",
    "        else:\n",
    "            trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                                     batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                                     batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "        return trainloader, validloader\n",
    "\n",
    "    def update_weights(self, model, global_round, learning_rate, local_epochs, optimizer):\n",
    "        # Set mode to train model\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "\n",
    "        # Set optimizer for the local updates\n",
    "        if optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,\n",
    "                                        ) # momentum=0.5\n",
    "        elif optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                         weight_decay=1e-4)\n",
    "\n",
    "        for i in range(local_epochs):\n",
    "            batch_loss = []\n",
    "            for batch_idx, (features, labels, sensitive) in enumerate(self.trainloader):\n",
    "                features, labels = features.to(DEVICE), labels.to(DEVICE).type(torch.LongTensor)\n",
    "                # we need to set the gradients to zero before starting to do backpropragation \n",
    "                # because PyTorch accumulates the gradients on subsequent backward passes. \n",
    "                # This is convenient while training RNNs\n",
    "                \n",
    "                log_probs, logits = model(features)\n",
    "                loss, _, _ = loss_func(self.option,\n",
    "                    logits, labels, logits, sensitive, mean_sensitive, self.penalty)\n",
    "                    \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch_idx % 50 == 0:\n",
    "                    print('| Global Round : {} | Local Epoch : {} | [{}/{} ({:.0f}%)]\\tBatch Loss: {:.6f}'.format(\n",
    "                        global_round, i, batch_idx * len(features),\n",
    "                        len(self.trainloader.dataset),\n",
    "                        100. * batch_idx / len(self.trainloader), loss.item()))\n",
    "                batch_loss.append(loss.item())\n",
    "            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "\n",
    "        # weight, loss\n",
    "        return model.state_dict(), sum(epoch_loss) / len(epoch_loss)\n",
    "\n",
    "    def inference(self, model, option):\n",
    "        \"\"\" \n",
    "        Returns the inference accuracy, \n",
    "                                loss, \n",
    "                                N(sensitive group, pos), \n",
    "                                N(non-sensitive group, pos), \n",
    "                                N(sensitive group),\n",
    "                                N(non-sensitive group),\n",
    "                                acc_loss,\n",
    "                                fair_loss\n",
    "        \"\"\"\n",
    "\n",
    "        model.eval()\n",
    "        loss, total, correct, fair_loss, acc_loss, num_batch = 0.0, 0.0, 0.0, 0.0, 0.0, 0\n",
    "        n_yz = {(0,0):0, (0,1):0, (1,0):0, (1,1):0}\n",
    "        loss_yz = {(0,0):0, (0,1):0, (1,0):0, (1,1):0}\n",
    "        \n",
    "        dataset = self.validloader if option != \"FairBatch\" else self.dataset\n",
    "        for batch_idx, (features, labels, sensitive) in enumerate(self.validloader):\n",
    "            features, labels = features.to(DEVICE), labels.to(DEVICE).type(torch.LongTensor)\n",
    "            sensitive = sensitive.to(DEVICE)\n",
    "            \n",
    "            # Inference\n",
    "            outputs, logits = model(features)\n",
    "\n",
    "            # Prediction\n",
    "            _, pred_labels = torch.max(outputs, 1)\n",
    "            pred_labels = pred_labels.view(-1)\n",
    "            bool_correct = torch.eq(pred_labels, labels)\n",
    "            correct += torch.sum(bool_correct).item()\n",
    "            total += len(labels)\n",
    "            num_batch += 1\n",
    "            \n",
    "            group_boolean_idx = {}\n",
    "             \n",
    "#             # classified negative, nonsensitive\n",
    "#             group_boolean_idx[(0,0)] = torch.logical_and(torch.logical_not(pred_labels), torch.logical_not(sensitive))\n",
    "#             # classified negative, sensitive\n",
    "#             group_boolean_idx[(0,1)] = torch.logical_and(torch.logical_not(pred_labels), sensitive)\n",
    "#             # classified positive, nonsensitive\n",
    "#             group_boolean_idx[(1,0)] = torch.logical_and(pred_labels, torch.logical_not(sensitive))\n",
    "#             # classified positive, sensitive\n",
    "#             group_boolean_idx[(1,1)] = torch.logical_and(pred_labels, sensitive)\n",
    "            \n",
    "            \n",
    "            for yz in n_yz:\n",
    "                group_boolean_idx[yz] = (pred_labels == yz[0]) & (sensitive == yz[1])\n",
    "                n_yz[yz] += torch.sum(group_boolean_idx[yz]).item()            \n",
    "                \n",
    "                if self.option == \"FairBatch\":\n",
    "                # the objective function have no lagrangian term\n",
    "                    loss_yz_,_,_ = loss_func(\"unconstrained\", outputs[group_boolean_idx[yz]], \n",
    "                                                    labels[group_boolean_idx[yz]], \n",
    "                                         logits[group_boolean_idx[yz]], sensitive[group_boolean_idx[yz]], \n",
    "                                         mean_sensitive, self.penalty)\n",
    "                    loss_yz[yz] += loss_yz_\n",
    "            \n",
    "            batch_loss, batch_acc_loss, batch_fair_loss = loss_func(self.option, outputs, \n",
    "                                                        labels, logits, sensitive, mean_sensitive, self.penalty)\n",
    "            loss, acc_loss, fair_loss = (loss + batch_loss.item(), \n",
    "                                         acc_loss + batch_acc_loss.item(), \n",
    "                                         fair_loss + batch_fair_loss.item())\n",
    "        accuracy = correct/total\n",
    "        if option == \"FairBatch\":\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, loss_yz\n",
    "        else:\n",
    "            return accuracy, loss, n_yz, acc_loss / num_batch, fair_loss / num_batch, None\n",
    "\n",
    "\n",
    "def test_inference(model, test_dataset, batch_size):\n",
    "    \"\"\" Returns the test accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    n_yz = {(0,0):0, (0,1):0, (1,0):0, (1,1):0}\n",
    "    \n",
    "    criterion = nn.NLLLoss().to(DEVICE)\n",
    "    testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
    "                            shuffle=False)\n",
    "\n",
    "    for batch_idx, (features, labels, sensitive) in enumerate(testloader):\n",
    "        features = features.to(DEVICE)\n",
    "        labels =  labels.to(DEVICE).type(torch.LongTensor)\n",
    "        # Inference\n",
    "        outputs, logits = model(features)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        bool_correct = torch.eq(pred_labels, labels)\n",
    "        correct += torch.sum(bool_correct).item()\n",
    "        total += len(labels)\n",
    "\n",
    "        # classified negative, nonsensitive\n",
    "        n_yz[(0,0)] += torch.sum(torch.logical_and(torch.logical_not(pred_labels), torch.logical_not(sensitive))).item()\n",
    "        # classified negative, sensitive\n",
    "        n_yz[(0,1)] += torch.sum(torch.logical_and(torch.logical_not(pred_labels), sensitive)).item()\n",
    "        # classified positive, nonsensitive\n",
    "        n_yz[(1,0)] += torch.sum(torch.logical_and(pred_labels, torch.logical_not(sensitive))).item()\n",
    "        # classified positive, sensitive\n",
    "        n_yz[(1,1)] += torch.sum(torch.logical_and(pred_labels, sensitive)).item()\n",
    "\n",
    "    accuracy = correct/total\n",
    "    # |P(Group1, pos) - P(Group2, pos)| = |N(Group1, pos)/N(Group1) - N(Group2, pos)/N(Group2)|\n",
    "    return accuracy, loss, RD(n_yz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, option = \"unconstrained\", batch_size = 128, num_clients = 2,\n",
    "          num_rounds = 5, learning_rate = 0.01, optimizer = 'adam', local_epochs= 5, \n",
    "          num_workers = 4, print_every = 1,\n",
    "         penalty = 1, alpha = 0.005):\n",
    "    \"\"\"\n",
    "    Server execution.\n",
    "    \"\"\"\n",
    "    # Training\n",
    "    train_loss, train_accuracy = [], []\n",
    "    val_acc_list, net_list = [], []\n",
    "    cv_loss, cv_acc = [], []\n",
    "    val_loss_pre, counter = 0, 0\n",
    "    start_time = time.time()\n",
    "    weights = model.state_dict()\n",
    "    \n",
    "    test_loader = DataLoader(dataset = test_dataset,\n",
    "                            batch_size = batch_size,\n",
    "                            num_workers = num_workers)\n",
    "    \n",
    "    train_loader = DataLoader(dataset = train_dataset,\n",
    "                        batch_size = batch_size,\n",
    "                        num_workers = num_workers)\n",
    "\n",
    "    def average_weights(w):\n",
    "        \"\"\"\n",
    "        Returns the average of the weights.\n",
    "        \"\"\"\n",
    "        w_avg = copy.deepcopy(w[0])\n",
    "        for key in w_avg.keys():\n",
    "            for i in range(1, len(w)):\n",
    "                w_avg[key] += w[i][key]\n",
    "            w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "        return w_avg\n",
    "\n",
    "    # the number of samples whose label is y and sensitive attribute is z\n",
    "    m_yz = {(0,0): ((train_dataset.y == 0) & (train_dataset.sen == 0)).sum(),\n",
    "           (1,0): ((train_dataset.y == 1) & (train_dataset.sen == 0)).sum(),\n",
    "           (0,1): ((train_dataset.y == 0) & (train_dataset.sen == 1)).sum(),\n",
    "           (1,1): ((train_dataset.y == 1) & (train_dataset.sen == 1)).sum()}\n",
    "    \n",
    "    lbd = {\n",
    "        (0,0): m_yz[(0,0)]/train_dataset.y.shape[0], \n",
    "        (0,1): m_yz[(0,1)]/train_dataset.y.shape[0],\n",
    "        (1,0): m_yz[(0,1)]/train_dataset.y.shape[0],\n",
    "        (1,1): m_yz[(1,1)]/train_dataset.y.shape[0],\n",
    "          }\n",
    "    \n",
    "    for round_ in tqdm(range(num_rounds)):\n",
    "        local_weights, local_losses = [], []\n",
    "        print(f'\\n | Global Training Round : {round_+1} |\\n')\n",
    "\n",
    "        model.train()\n",
    "        m = 2 # the number of clients to be chosen in each round_\n",
    "        idxs_users = np.random.choice(range(num_clients), m, replace=False)\n",
    "\n",
    "        for idx in idxs_users:\n",
    "            local_model = ClientUpdate(dataset=train_dataset,\n",
    "                                        idxs=clients_idx[idx], batch_size = batch_size, \n",
    "                                       option = option, penalty = penalty, lbd = lbd)\n",
    "            w, loss = local_model.update_weights(\n",
    "                            model=copy.deepcopy(model), global_round=round_, \n",
    "                                learning_rate = learning_rate, local_epochs = local_epochs, \n",
    "                                optimizer = optimizer)\n",
    "            local_weights.append(copy.deepcopy(w))\n",
    "            local_losses.append(copy.deepcopy(loss))\n",
    "\n",
    "        # update global weights\n",
    "        weights = average_weights(local_weights)\n",
    "        model.load_state_dict(weights)\n",
    "\n",
    "        loss_avg = sum(local_losses) / len(local_losses)\n",
    "        train_loss.append(loss_avg)\n",
    "\n",
    "        # Calculate avg training accuracy over all clients at every round\n",
    "        list_acc, list_loss = [], []\n",
    "        # the number of samples which are assigned to class y and belong to the sensitive group z\n",
    "        n_yz = {(0,0):0, (0,1):0, (1,0):0, (1,1):0}\n",
    "        loss_yz = {(0,0):0, (0,1):0, (1,0):0, (1,1):0}\n",
    "        model.eval()\n",
    "        for c in range(m):\n",
    "            local_model = ClientUpdate(dataset=train_dataset,\n",
    "                                        idxs=clients_idx[c], batch_size = batch_size, option = option, \n",
    "                                       penalty = penalty, lbd = lbd)\n",
    "            # validation dataset inference\n",
    "            acc, loss, n_yz_c, acc_loss, fair_loss, loss_yz_c = local_model.inference(model = model, \n",
    "                                                                                      option = option) \n",
    "            list_acc.append(acc)\n",
    "            list_loss.append(loss)\n",
    "            \n",
    "            for yz in n_yz:\n",
    "                n_yz[yz] += n_yz_c[yz]\n",
    "                \n",
    "                if option == \"FairBatch\": loss_yz[yz] += loss_yz_c[yz]\n",
    "                \n",
    "            print(\"Client %d: accuracy loss: %.2f | fairness loss %.2f | RD = %.2f = |%d/%d-%d/%d| \" % (\n",
    "                c, acc_loss, fair_loss, RD(n_yz_c), n_yz_c[(1,1)], n_yz_c[(1,1)] + n_yz_c[(0,1)], \n",
    "                n_yz_c[(1,0)], n_yz_c[(1,0)] + n_yz_c[(0,0)]))\n",
    "            \n",
    "        if option == \"FairBatch\": # update the lambda\n",
    "            if abs(loss_yz[(0,0)]/m_yz[(0,0)] - loss_yz[(1,0)/m_yz[(1,0)]]) >= \\\n",
    "                abs(loss_yz[(0,1)]/m_yz[(0,1)] - loss_yz[(1,1)]/m_yz[(1,1)]):\n",
    "                lbd[(0,0)] -= alpha * (2*int((loss_yz[(0,0)]/m_yz[(0,0)] - loss_yz[(1,0)/m_yz[(1,0)]]) > 0)-1)\n",
    "                lbd[(0,1)] = (m_yz[(0,0)] + m_yz[(0,1)])/train_dataset.y.shape[0] - lbd[(0,0)]\n",
    "            else:\n",
    "                lbd[(1,0)] -= alpha * (2*int((loss_yz[(0,1)]/m_yz[(0,1)] - loss_yz[(1,1)]/m_yz[(1,1)]) > 0)-1)\n",
    "                lbd[(1,1)] = (m_yz[(1,0)] + m_yz[(1,1)])/train_dataset.y.shape[0] - lbd[(1,0)]\n",
    "            \n",
    "        train_accuracy.append(sum(list_acc)/len(list_acc))\n",
    "\n",
    "        # print global training loss after every 'i' rounds\n",
    "        if (round_+1) % print_every == 0:\n",
    "            print(f' \\nAvg Training Stats after {round_+1} global rounds:')\n",
    "            if option != \"FairBatch\":\n",
    "                print(\"Training loss: %.2f | Validation accuracy: %.2f%% | Validation RD: %.2f\" % (\n",
    "                     np.mean(np.array(train_loss)), \n",
    "                    100*train_accuracy[-1],\n",
    "                    RD(n_yz)\n",
    "                     ))\n",
    "            else:\n",
    "                print(\"Training loss: %.2f | Training accuracy: %.2f%% | Training RD: %.2f\" % (\n",
    "                     np.mean(np.array(train_loss)), \n",
    "                    100*train_accuracy[-1],\n",
    "                    RD(n_yz)\n",
    "                     ))\n",
    "\n",
    "    # Test inference after completion of training\n",
    "    test_acc, test_loss, rd= test_inference(model, test_dataset, batch_size)\n",
    "\n",
    "    print(f' \\n Results after {num_rounds} global rounds of training:')\n",
    "    print(\"|---- Avg Train Accuracy: {:.2f}%\".format(100*train_accuracy[-1]))\n",
    "    print(\"|---- Test Accuracy: {:.2f}%\".format(100*test_acc))\n",
    "\n",
    "    # Compute RD: risk difference - fairness metric\n",
    "    # |P(Group1, pos) - P(Group2, pos)| = |N(Group1, pos)/N(Group1) - N(Group2, pos)/N(Group2)|\n",
    "    print(\"|---- Test RD: {:.2f}\".format(rd))\n",
    "\n",
    "    print('\\n Total Run Time: {0:0.4f} sec'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "1\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11128,)\n",
      "(12192,)\n",
      "(11128,)\n",
      "(12192,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ad2fbe497139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogReg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_FEATURES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"FairBatch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-07f22ccb2748>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, option, batch_size, num_clients, num_rounds, learning_rate, optimizer, local_epochs, num_workers, print_every, penalty, alpha)\u001b[0m\n\u001b[1;32m     61\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mround_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                                 \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                                 optimizer = optimizer)\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mlocal_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mlocal_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-138fd87f806d>\u001b[0m in \u001b[0;36mupdate_weights\u001b[0;34m(self, model, global_round, learning_rate, local_epochs, optimizer)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitive\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# we need to set the gradients to zero before starting to do backpropragation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Google Drive/办公/research/toyexperiment/Fair-FedAvg/utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msensitive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "train(logReg(num_features=NUM_FEATURES, num_classes=2), option = \"FairBatch\", optimizer = 'sgd', learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 85.701195\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 95.518326\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 55.537357\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 124.769241\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 57.493580\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 64.346741\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 39.510857\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 98.788841\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 43.079445\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 52.270847\n",
      "| Global Round : 0 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 63.154572\n",
      "| Global Round : 0 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 51.702747\n",
      "| Global Round : 0 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 51.218628\n",
      "| Global Round : 0 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 98.094604\n",
      "| Global Round : 0 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 45.423496\n",
      "| Global Round : 0 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 48.185802\n",
      "| Global Round : 0 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 61.872990\n",
      "| Global Round : 0 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 84.160576\n",
      "| Global Round : 0 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 53.814198\n",
      "| Global Round : 0 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 62.522629\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 86.539497\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 65.617279\n",
      "| Global Round : 0 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 61.829472\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 58.726238\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 69.184311\n",
      "| Global Round : 0 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 92.064110\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 81.158455\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 132.182648\n",
      "| Global Round : 0 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 101.901314\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 54.408318\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 70.676003\n",
      "| Global Round : 0 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 124.276604\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 60.963821\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 59.285587\n",
      "| Global Round : 0 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 42.060745\n",
      "| Global Round : 0 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 72.127525\n",
      "| Global Round : 0 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 49.946194\n",
      "| Global Round : 0 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 86.482803\n",
      "| Global Round : 0 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 71.105095\n",
      "| Global Round : 0 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 84.184929\n",
      "| Global Round : 0 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 79.573471\n",
      "| Global Round : 0 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 46.301956\n",
      "| Global Round : 0 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 104.714264\n",
      "| Global Round : 0 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 36.732529\n",
      "| Global Round : 0 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 49.941154\n",
      "| Global Round : 0 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 95.759537\n",
      "| Global Round : 0 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 49.569244\n",
      "| Global Round : 0 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 64.540077\n",
      "| Global Round : 0 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 37.265095\n",
      "| Global Round : 0 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 46.748829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:08<00:33,  8.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 106.64 | fairness loss 1.42 | RD = 0.28 = |102/592-645/1421| \n",
      "Client 1: accuracy loss: 63.24 | fairness loss 2.21 | RD = 0.22 = |63/421-308/823| \n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: 80.82 | Validation accuracy: 81.00% | Validation RD: 0.26\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 56.406567\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 76.393646\n",
      "| Global Round : 1 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 130.139145\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 47.605862\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 69.210442\n",
      "| Global Round : 1 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 38.727039\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 50.772484\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 96.618500\n",
      "| Global Round : 1 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 94.737389\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 54.686928\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 62.247200\n",
      "| Global Round : 1 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 69.687195\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 39.998142\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 89.298813\n",
      "| Global Round : 1 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 72.421036\n",
      "| Global Round : 1 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 54.239193\n",
      "| Global Round : 1 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 83.863747\n",
      "| Global Round : 1 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 105.546280\n",
      "| Global Round : 1 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 53.863182\n",
      "| Global Round : 1 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 65.877213\n",
      "| Global Round : 1 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 61.535336\n",
      "| Global Round : 1 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 46.831066\n",
      "| Global Round : 1 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 126.386169\n",
      "| Global Round : 1 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 94.640106\n",
      "| Global Round : 1 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 74.056290\n",
      "| Global Round : 1 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 148.060898\n",
      "| Global Round : 1 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 57.309158\n",
      "| Global Round : 1 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 35.562958\n",
      "| Global Round : 1 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 101.277100\n",
      "| Global Round : 1 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 62.831280\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 53.544636\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 40.332825\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 41.385170\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 60.043808\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 54.254608\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 93.334877\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 51.579082\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 106.875244\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 48.106781\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 83.391510\n",
      "| Global Round : 1 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 67.073219\n",
      "| Global Round : 1 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 114.655441\n",
      "| Global Round : 1 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 51.405876\n",
      "| Global Round : 1 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 67.198517\n",
      "| Global Round : 1 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 37.443542\n",
      "| Global Round : 1 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 76.393463\n",
      "| Global Round : 1 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 38.867428\n",
      "| Global Round : 1 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 112.107437\n",
      "| Global Round : 1 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 47.531967\n",
      "| Global Round : 1 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 47.840996\n",
      "Client 0: accuracy loss: 104.31 | fairness loss 1.37 | RD = 0.28 = |70/592-559/1421| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:16<00:24,  8.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: accuracy loss: 62.52 | fairness loss 1.59 | RD = 0.24 = |41/421-277/823| \n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training loss: 75.97 | Validation accuracy: 83.04% | Validation RD: 0.26\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 50.084232\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 70.881454\n",
      "| Global Round : 2 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 68.897469\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 47.279171\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 98.653030\n",
      "| Global Round : 2 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 71.950020\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 53.359856\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 46.227318\n",
      "| Global Round : 2 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 50.440899\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 31.424915\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 51.781395\n",
      "| Global Round : 2 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 102.605118\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 54.343422\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 69.985550\n",
      "| Global Round : 2 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 57.422791\n",
      "| Global Round : 2 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 35.913807\n",
      "| Global Round : 2 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 71.349022\n",
      "| Global Round : 2 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 45.287941\n",
      "| Global Round : 2 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 48.535110\n",
      "| Global Round : 2 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 55.986900\n",
      "| Global Round : 2 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 80.843025\n",
      "| Global Round : 2 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 39.678398\n",
      "| Global Round : 2 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 47.028229\n",
      "| Global Round : 2 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 83.842926\n",
      "| Global Round : 2 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 68.886002\n",
      "| Global Round : 2 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 45.122612\n",
      "| Global Round : 2 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 70.719994\n",
      "| Global Round : 2 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 36.305687\n",
      "| Global Round : 2 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 103.674545\n",
      "| Global Round : 2 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 100.420944\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 40.814377\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 127.494110\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 48.297661\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 87.155838\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 44.214764\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 86.262352\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 42.689438\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 36.141056\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 41.193291\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 49.286537\n",
      "| Global Round : 2 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 40.249374\n",
      "| Global Round : 2 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 57.869385\n",
      "| Global Round : 2 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 47.701221\n",
      "| Global Round : 2 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 81.782120\n",
      "| Global Round : 2 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 61.282692\n",
      "| Global Round : 2 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 78.073242\n",
      "| Global Round : 2 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 46.673008\n",
      "| Global Round : 2 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 45.195831\n",
      "| Global Round : 2 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 44.746063\n",
      "| Global Round : 2 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 47.392059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:23<00:15,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 106.91 | fairness loss 0.28 | RD = 0.26 = |92/592-591/1421| \n",
      "Client 1: accuracy loss: 64.63 | fairness loss 0.33 | RD = 0.25 = |53/421-306/823| \n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training loss: 74.04 | Validation accuracy: 82.76% | Validation RD: 0.26\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 50.219044\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 108.420769\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 56.948952\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 76.701958\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 38.305408\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 55.711815\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 44.169247\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 53.509918\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 57.385235\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 62.559669\n",
      "| Global Round : 3 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 60.024792\n",
      "| Global Round : 3 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 59.182133\n",
      "| Global Round : 3 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 48.828854\n",
      "| Global Round : 3 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 111.383636\n",
      "| Global Round : 3 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 44.699898\n",
      "| Global Round : 3 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 44.327805\n",
      "| Global Round : 3 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 48.687820\n",
      "| Global Round : 3 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 64.002937\n",
      "| Global Round : 3 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 53.450565\n",
      "| Global Round : 3 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 55.363274\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 39.982674\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 44.108028\n",
      "| Global Round : 3 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 106.168602\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 46.054886\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 50.657974\n",
      "| Global Round : 3 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 53.921516\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 44.568745\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 69.916275\n",
      "| Global Round : 3 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 89.861816\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 54.828773\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 58.977970\n",
      "| Global Round : 3 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 71.666061\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 50.958855\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 58.732136\n",
      "| Global Round : 3 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 53.474339\n",
      "| Global Round : 3 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 47.377373\n",
      "| Global Round : 3 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 59.669498\n",
      "| Global Round : 3 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 68.726257\n",
      "| Global Round : 3 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 44.244850\n",
      "| Global Round : 3 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 67.684189\n",
      "| Global Round : 3 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 69.097298\n",
      "| Global Round : 3 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 72.380692\n",
      "| Global Round : 3 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 45.302082\n",
      "| Global Round : 3 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 61.225410\n",
      "| Global Round : 3 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 48.947292\n",
      "| Global Round : 3 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 101.245712\n",
      "| Global Round : 3 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 38.242783\n",
      "| Global Round : 3 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 85.621239\n",
      "| Global Round : 3 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 58.419434\n",
      "| Global Round : 3 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 39.076134\n",
      "Client 0: accuracy loss: 104.23 | fairness loss 0.37 | RD = 0.19 = |76/592-448/1421| \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:31<00:07,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1: accuracy loss: 63.17 | fairness loss 0.38 | RD = 0.19 = |41/421-235/823| \n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training loss: 72.36 | Validation accuracy: 83.86% | Validation RD: 0.19\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 37.441402\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 91.750023\n",
      "| Global Round : 4 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 85.341866\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 52.610249\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 100.512161\n",
      "| Global Round : 4 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 89.618340\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 37.983765\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 36.879108\n",
      "| Global Round : 4 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 75.845375\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 52.246368\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 76.030319\n",
      "| Global Round : 4 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 64.301758\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 34.575394\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 63.519913\n",
      "| Global Round : 4 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 97.129852\n",
      "| Global Round : 4 | Local Epoch : 5 | [0/18116 (0%)]\tBatch Loss: 43.587822\n",
      "| Global Round : 4 | Local Epoch : 5 | [6400/18116 (35%)]\tBatch Loss: 89.152077\n",
      "| Global Round : 4 | Local Epoch : 5 | [12800/18116 (70%)]\tBatch Loss: 29.313646\n",
      "| Global Round : 4 | Local Epoch : 6 | [0/18116 (0%)]\tBatch Loss: 38.771362\n",
      "| Global Round : 4 | Local Epoch : 6 | [6400/18116 (35%)]\tBatch Loss: 35.979725\n",
      "| Global Round : 4 | Local Epoch : 6 | [12800/18116 (70%)]\tBatch Loss: 111.153435\n",
      "| Global Round : 4 | Local Epoch : 7 | [0/18116 (0%)]\tBatch Loss: 72.174828\n",
      "| Global Round : 4 | Local Epoch : 7 | [6400/18116 (35%)]\tBatch Loss: 45.938396\n",
      "| Global Round : 4 | Local Epoch : 7 | [12800/18116 (70%)]\tBatch Loss: 41.045540\n",
      "| Global Round : 4 | Local Epoch : 8 | [0/18116 (0%)]\tBatch Loss: 54.875828\n",
      "| Global Round : 4 | Local Epoch : 8 | [6400/18116 (35%)]\tBatch Loss: 55.881565\n",
      "| Global Round : 4 | Local Epoch : 8 | [12800/18116 (70%)]\tBatch Loss: 53.171135\n",
      "| Global Round : 4 | Local Epoch : 9 | [0/18116 (0%)]\tBatch Loss: 39.335236\n",
      "| Global Round : 4 | Local Epoch : 9 | [6400/18116 (35%)]\tBatch Loss: 57.849045\n",
      "| Global Round : 4 | Local Epoch : 9 | [12800/18116 (70%)]\tBatch Loss: 68.259827\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 63.853996\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 56.605316\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 60.260368\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 52.794441\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 44.661343\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 131.325470\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 44.052860\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 67.613190\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 46.167023\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 44.062660\n",
      "| Global Round : 4 | Local Epoch : 5 | [0/11188 (0%)]\tBatch Loss: 52.414543\n",
      "| Global Round : 4 | Local Epoch : 5 | [6400/11188 (57%)]\tBatch Loss: 67.295830\n",
      "| Global Round : 4 | Local Epoch : 6 | [0/11188 (0%)]\tBatch Loss: 60.757946\n",
      "| Global Round : 4 | Local Epoch : 6 | [6400/11188 (57%)]\tBatch Loss: 65.432556\n",
      "| Global Round : 4 | Local Epoch : 7 | [0/11188 (0%)]\tBatch Loss: 51.724796\n",
      "| Global Round : 4 | Local Epoch : 7 | [6400/11188 (57%)]\tBatch Loss: 66.712051\n",
      "| Global Round : 4 | Local Epoch : 8 | [0/11188 (0%)]\tBatch Loss: 44.826580\n",
      "| Global Round : 4 | Local Epoch : 8 | [6400/11188 (57%)]\tBatch Loss: 50.497223\n",
      "| Global Round : 4 | Local Epoch : 9 | [0/11188 (0%)]\tBatch Loss: 47.488506\n",
      "| Global Round : 4 | Local Epoch : 9 | [6400/11188 (57%)]\tBatch Loss: 45.385395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:38<00:00,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 103.62 | fairness loss 1.38 | RD = 0.25 = |78/592-547/1421| \n",
      "Client 1: accuracy loss: 62.41 | fairness loss 1.53 | RD = 0.26 = |40/421-296/823| \n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training loss: 71.58 | Validation accuracy: 83.57% | Validation RD: 0.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 5 global rounds of training:\n",
      "|---- Avg Train Accuracy: 83.57%\n",
      "|---- Test Accuracy: 84.37%\n",
      "|---- Test RD: 0.20\n",
      "\n",
      " Total Run Time: 39.1036 sec\n"
     ]
    }
   ],
   "source": [
    "train(logReg(num_features=NUM_FEATURES, num_classes=2), \n",
    "      \"Zafar\", penalty = 50, optimizer = 'sgd', learning_rate = 0.01,\n",
    "     num_rounds = 5, local_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " | Global Training Round : 1 |\n",
      "\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 90.584633\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 33.299267\n",
      "| Global Round : 0 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 34.747501\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 52.169621\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 49.746605\n",
      "| Global Round : 0 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 39.635277\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 33.474205\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 56.598858\n",
      "| Global Round : 0 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 47.075012\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 43.604736\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 42.095608\n",
      "| Global Round : 0 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 39.268177\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 47.453789\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 39.124786\n",
      "| Global Round : 0 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 49.712803\n",
      "| Global Round : 0 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 89.439316\n",
      "| Global Round : 0 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 67.822891\n",
      "| Global Round : 0 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 50.131737\n",
      "| Global Round : 0 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 58.392162\n",
      "| Global Round : 0 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 46.015633\n",
      "| Global Round : 0 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 49.319641\n",
      "| Global Round : 0 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 49.747967\n",
      "| Global Round : 0 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 40.073963\n",
      "| Global Round : 0 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 47.261253\n",
      "| Global Round : 0 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 55.405338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 1/5 [00:03<00:15,  3.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 107.20 | fairness loss 0.38 | RD = 0.38 = |120/592-824/1421| \n",
      "Client 1: accuracy loss: 62.68 | fairness loss 0.52 | RD = 0.31 = |72/421-399/823| \n",
      " \n",
      "Avg Training Stats after 1 global rounds:\n",
      "Training loss: 48.41 | Validation accuracy: 77.30% | Validation RD: 0.36\n",
      "\n",
      " | Global Training Round : 2 |\n",
      "\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 45.183014\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 35.370556\n",
      "| Global Round : 1 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 43.966156\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 39.527290\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 53.414135\n",
      "| Global Round : 1 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 39.270737\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 37.068527\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 53.278751\n",
      "| Global Round : 1 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 33.347046\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 30.195366\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 56.772560\n",
      "| Global Round : 1 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 38.890820\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 40.555466\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 46.269650\n",
      "| Global Round : 1 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 42.147408\n",
      "| Global Round : 1 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 50.799385\n",
      "| Global Round : 1 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 55.671265\n",
      "| Global Round : 1 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 43.037373\n",
      "| Global Round : 1 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 46.082642\n",
      "| Global Round : 1 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 43.316387\n",
      "| Global Round : 1 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 42.134773\n",
      "| Global Round : 1 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 52.746258\n",
      "| Global Round : 1 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 40.649818\n",
      "| Global Round : 1 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 44.861332\n",
      "| Global Round : 1 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 52.346836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 2/5 [00:07<00:11,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 103.17 | fairness loss 0.50 | RD = 0.32 = |99/592-698/1421| \n",
      "Client 1: accuracy loss: 60.82 | fairness loss 0.63 | RD = 0.28 = |57/421-345/823| \n",
      " \n",
      "Avg Training Stats after 2 global rounds:\n",
      "Training loss: 46.98 | Validation accuracy: 80.71% | Validation RD: 0.31\n",
      "\n",
      " | Global Training Round : 3 |\n",
      "\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 48.978443\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 45.512356\n",
      "| Global Round : 2 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 42.914787\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 30.520506\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 34.681450\n",
      "| Global Round : 2 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 63.349865\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 34.125118\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 39.767441\n",
      "| Global Round : 2 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 64.989754\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 43.954468\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 38.210529\n",
      "| Global Round : 2 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 51.175667\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 44.250629\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 37.762714\n",
      "| Global Round : 2 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 35.186085\n",
      "| Global Round : 2 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 54.473392\n",
      "| Global Round : 2 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 57.278252\n",
      "| Global Round : 2 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 46.457642\n",
      "| Global Round : 2 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 60.701427\n",
      "| Global Round : 2 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 41.634487\n",
      "| Global Round : 2 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 47.398609\n",
      "| Global Round : 2 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 35.615982\n",
      "| Global Round : 2 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 56.106770\n",
      "| Global Round : 2 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 46.125065\n",
      "| Global Round : 2 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 44.838764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 3/5 [00:11<00:07,  3.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 100.04 | fairness loss 0.65 | RD = 0.28 = |70/592-565/1421| \n",
      "Client 1: accuracy loss: 59.37 | fairness loss 0.78 | RD = 0.26 = |38/421-291/823| \n",
      " \n",
      "Avg Training Stats after 3 global rounds:\n",
      "Training loss: 46.24 | Validation accuracy: 83.17% | Validation RD: 0.27\n",
      "\n",
      " | Global Training Round : 4 |\n",
      "\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 40.749920\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 41.214054\n",
      "| Global Round : 3 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 31.918589\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 41.752522\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 45.724602\n",
      "| Global Round : 3 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 35.968750\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 46.231560\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 34.507687\n",
      "| Global Round : 3 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 42.702511\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 45.213066\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 41.373875\n",
      "| Global Round : 3 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 50.345791\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 34.850914\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 40.225533\n",
      "| Global Round : 3 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 62.664192\n",
      "| Global Round : 3 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 48.385723\n",
      "| Global Round : 3 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 47.576580\n",
      "| Global Round : 3 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 48.179001\n",
      "| Global Round : 3 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 46.929192\n",
      "| Global Round : 3 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 40.470497\n",
      "| Global Round : 3 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 35.964291\n",
      "| Global Round : 3 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 50.204704\n",
      "| Global Round : 3 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 51.090797\n",
      "| Global Round : 3 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 45.071957\n",
      "| Global Round : 3 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 46.363907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 4/5 [00:15<00:03,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 101.32 | fairness loss 0.59 | RD = 0.33 = |86/592-672/1421| \n",
      "Client 1: accuracy loss: 60.19 | fairness loss 0.69 | RD = 0.29 = |53/421-345/823| \n",
      " \n",
      "Avg Training Stats after 4 global rounds:\n",
      "Training loss: 45.84 | Validation accuracy: 81.32% | Validation RD: 0.32\n",
      "\n",
      " | Global Training Round : 5 |\n",
      "\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/18116 (0%)]\tBatch Loss: 33.437969\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/18116 (35%)]\tBatch Loss: 44.129021\n",
      "| Global Round : 4 | Local Epoch : 0 | [12800/18116 (70%)]\tBatch Loss: 32.441113\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/18116 (0%)]\tBatch Loss: 32.437141\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/18116 (35%)]\tBatch Loss: 47.204319\n",
      "| Global Round : 4 | Local Epoch : 1 | [12800/18116 (70%)]\tBatch Loss: 45.764359\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/18116 (0%)]\tBatch Loss: 35.096237\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/18116 (35%)]\tBatch Loss: 42.302170\n",
      "| Global Round : 4 | Local Epoch : 2 | [12800/18116 (70%)]\tBatch Loss: 34.989052\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/18116 (0%)]\tBatch Loss: 29.948671\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/18116 (35%)]\tBatch Loss: 41.860737\n",
      "| Global Round : 4 | Local Epoch : 3 | [12800/18116 (70%)]\tBatch Loss: 30.050640\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/18116 (0%)]\tBatch Loss: 31.818878\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/18116 (35%)]\tBatch Loss: 40.868984\n",
      "| Global Round : 4 | Local Epoch : 4 | [12800/18116 (70%)]\tBatch Loss: 38.819519\n",
      "| Global Round : 4 | Local Epoch : 0 | [0/11188 (0%)]\tBatch Loss: 43.912750\n",
      "| Global Round : 4 | Local Epoch : 0 | [6400/11188 (57%)]\tBatch Loss: 48.607704\n",
      "| Global Round : 4 | Local Epoch : 1 | [0/11188 (0%)]\tBatch Loss: 42.275261\n",
      "| Global Round : 4 | Local Epoch : 1 | [6400/11188 (57%)]\tBatch Loss: 54.594593\n",
      "| Global Round : 4 | Local Epoch : 2 | [0/11188 (0%)]\tBatch Loss: 49.982903\n",
      "| Global Round : 4 | Local Epoch : 2 | [6400/11188 (57%)]\tBatch Loss: 42.703381\n",
      "| Global Round : 4 | Local Epoch : 3 | [0/11188 (0%)]\tBatch Loss: 56.243759\n",
      "| Global Round : 4 | Local Epoch : 3 | [6400/11188 (57%)]\tBatch Loss: 57.572933\n",
      "| Global Round : 4 | Local Epoch : 4 | [0/11188 (0%)]\tBatch Loss: 40.120544\n",
      "| Global Round : 4 | Local Epoch : 4 | [6400/11188 (57%)]\tBatch Loss: 44.566769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:19<00:00,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0: accuracy loss: 99.73 | fairness loss 0.63 | RD = 0.24 = |82/592-536/1421| \n",
      "Client 1: accuracy loss: 59.55 | fairness loss 0.73 | RD = 0.23 = |48/421-285/823| \n",
      " \n",
      "Avg Training Stats after 5 global rounds:\n",
      "Training loss: 45.55 | Validation accuracy: 83.52% | Validation RD: 0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Results after 5 global rounds of training:\n",
      "|---- Avg Train Accuracy: 83.52%\n",
      "|---- Test Accuracy: 84.48%\n",
      "|---- Test RD: 0.19\n",
      "\n",
      " Total Run Time: 19.4675 sec\n"
     ]
    }
   ],
   "source": [
    "train(logReg(num_features=NUM_FEATURES, num_classes=2), optimizer = 'sgd', learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,3,4])[[1,2]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
