{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FR-Train on poisoned synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "from FRTrain_arch import Generator, DiscriminatorF, DiscriminatorR, weights_init_normal, test_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data (using poisoned y train label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a namespace object which contains some of the hyperparameters\n",
    "opt = Namespace(num_train=2000, num_val1=200, num_val2=500, num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = opt.num_train\n",
    "num_val1 = opt.num_val1\n",
    "num_val2 = opt.num_val2\n",
    "num_test = opt.num_test\n",
    "\n",
    "X = np.load('X_synthetic.npy') # Input features\n",
    "y = np.load('y_synthetic.npy') # Original labels\n",
    "y_poi = np.load('y_poi.npy') # Poisoned train labels\n",
    "s1 = np.load('s1_synthetic.npy') # Sensitive features\n",
    "\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.FloatTensor(y)\n",
    "y_poi = torch.FloatTensor(y_poi)\n",
    "s1 = torch.FloatTensor(s1)\n",
    "\n",
    "X_train = X[:num_train - num_val1]\n",
    "y_train = y_poi[:num_train - num_val1] # Poisoned label\n",
    "s1_train = s1[:num_train - num_val1]\n",
    "\n",
    "X_val = X[num_train: num_train + num_val1]\n",
    "y_val = y[num_train: num_train + num_val1]\n",
    "s1_val = s1[num_train: num_train + num_val1]\n",
    "\n",
    "# Currently not used\n",
    "# X_val2 = X[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "# y_val2 = y[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "# s1_val2 = s1[num_train + num_val1 : num_train + num_val1 + num_val2]\n",
    "\n",
    "X_test = X[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "y_test = y[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "s1_test = s1[num_train + num_val1 + num_val2 : num_train + num_val1 + num_val2 + num_test]\n",
    "\n",
    "XS_train = torch.cat([X_train, s1_train.reshape((s1_train.shape[0], 1))], dim=1)\n",
    "XS_val = torch.cat([X_val, s1_val.reshape((s1_val.shape[0], 1))], dim=1)\n",
    "XS_test = torch.cat([X_test, s1_test.reshape((s1_test.shape[0], 1))], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------- Number of Data -------------------------\n",
      "Train data : 1800, Validation data : 200, Test data : 1000 \n",
      "--------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--------------------- Number of Data -------------------------\" )\n",
    "print(\n",
    "    \"Train data : %d, Validation data : %d, Test data : %d \"\n",
    "    % (len(y_train), len(y_val), len(y_test))\n",
    ")       \n",
    "print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f, lambda_r, seed):\n",
    "    \"\"\"\n",
    "      Trains FR-Train by using the classes in FRTrain_arch.py.\n",
    "      \n",
    "      Args:\n",
    "        train_tensors: Training data.\n",
    "        val_tensors: Clean validation data.\n",
    "        test_tensors: Test data.\n",
    "        train_opt: Options for the training. It currently contains size of validation set, \n",
    "                number of epochs, generator/discriminator update ratio, and learning rates.\n",
    "        lambda_f: The tuning knob for L_2 (ref: FR-Train paper, Section 3.3).\n",
    "        lambda_r: The tuning knob for L_3 (ref: FR-Train paper, Section 3.3).\n",
    "        seed: An integer value for specifying torch random seed.\n",
    "        \n",
    "      Returns:\n",
    "        Information about the tuning knobs (lambda_f, lambda_r),\n",
    "        the test accuracy of the trained model, and disparate impact of the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    XS_train = train_tensors.XS_train\n",
    "    y_train = train_tensors.y_train\n",
    "    s1_train = train_tensors.s1_train\n",
    "    \n",
    "    XS_val = val_tensors.XS_val\n",
    "    y_val = val_tensors.y_val\n",
    "    s1_val = val_tensors.s1_val\n",
    "    \n",
    "    XS_test = test_tensors.XS_test\n",
    "    y_test = test_tensors.y_test\n",
    "    s1_test = test_tensors.s1_test\n",
    "    \n",
    "    # Saves return values here\n",
    "    test_result = [] \n",
    "    \n",
    "    val = train_opt.val # Number of data points in validation set\n",
    "    k = train_opt.k     # Update ratio of generator and discriminator (1:k training).\n",
    "    n_epochs = train_opt.n_epochs  # Number of training epoch\n",
    "    \n",
    "    # Changes the input validation data to an appropriate shape for the training\n",
    "    XSY_val = torch.cat([XS_val, y_val.reshape((y_val.shape[0], 1))], dim=1)  \n",
    "\n",
    "    # The loss values of each component will be saved in the following lists. \n",
    "    # We can draw epoch-loss graph by the following lists, if necessary.\n",
    "    g_losses =[]\n",
    "    d_f_losses = []\n",
    "    d_r_losses = []\n",
    "    clean_test_result = []\n",
    "\n",
    "    bce_loss = torch.nn.BCELoss()\n",
    "\n",
    "    # Initializes generator and discriminator\n",
    "    generator = Generator()\n",
    "    discriminator_F = DiscriminatorF()\n",
    "    discriminator_R = DiscriminatorR()\n",
    "\n",
    "    # Initializes weights\n",
    "    torch.manual_seed(seed)\n",
    "    generator.apply(weights_init_normal)\n",
    "    discriminator_F.apply(weights_init_normal)\n",
    "    discriminator_R.apply(weights_init_normal)\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=train_opt.lr_g)\n",
    "    optimizer_D_F = torch.optim.SGD(discriminator_F.parameters(), lr=train_opt.lr_f)\n",
    "    optimizer_D_R = torch.optim.SGD(discriminator_R.parameters(), lr=train_opt.lr_r)\n",
    "\n",
    "    XSY_val_data = XSY_val[:val]\n",
    "\n",
    "    train_len = XS_train.shape[0]\n",
    "    val_len = XSY_val.shape[0]\n",
    "\n",
    "    # Ground truths using in Disriminator_R\n",
    "    Tensor = torch.FloatTensor\n",
    "    valid = Variable(Tensor(train_len, 1).fill_(1.0), requires_grad=False)\n",
    "    generated = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    fake = Variable(Tensor(train_len, 1).fill_(0.0), requires_grad=False)\n",
    "    clean = Variable(Tensor(val_len, 1).fill_(1.0), requires_grad=False)\n",
    "    \n",
    "\n",
    "    r_weight = torch.ones_like(y_train, requires_grad=False).float()\n",
    "    r_ones = torch.ones_like(y_train, requires_grad=False).float()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # -------------------\n",
    "        #  Forwards Generator\n",
    "        # -------------------\n",
    "        if epoch % k == 0 or epoch < 500:\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "        gen_y = generator(XS_train)\n",
    "        gen_data = torch.cat([XS_train, gen_y.reshape((gen_y.shape[0], 1))], dim=1)\n",
    "\n",
    "\n",
    "        # -------------------------------\n",
    "        #  Trains Fairness Discriminator\n",
    "        # -------------------------------\n",
    "\n",
    "        optimizer_D_F.zero_grad()\n",
    "        \n",
    "        # Discriminator_F tries to distinguish the sensitive groups by using the output of the generator.\n",
    "        d_f_loss = bce_loss(discriminator_F(gen_y.detach()), s1_train)\n",
    "        d_f_loss.backward()\n",
    "        d_f_losses.append(d_f_loss)\n",
    "        optimizer_D_F.step()\n",
    "            \n",
    "            \n",
    "        # ---------------------------------\n",
    "        #  Trains Robustness Discriminator\n",
    "        # ---------------------------------\n",
    "\n",
    "        optimizer_D_R.zero_grad()\n",
    "\n",
    "        # Discriminator_R tries to distinguish whether the input is from the validation data or the generated data from generator.\n",
    "        clean_loss =  bce_loss(discriminator_R(XSY_val_data), clean)\n",
    "        poison_loss = bce_loss(discriminator_R(gen_data.detach()), fake)\n",
    "        d_r_loss = 0.5 * (clean_loss + poison_loss)\n",
    "\n",
    "        d_r_loss.backward()\n",
    "        d_r_losses.append(d_r_loss)\n",
    "        optimizer_D_R.step()\n",
    "\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Updates Generator\n",
    "        # ---------------------\n",
    "\n",
    "\n",
    "        if epoch < 500 :\n",
    "            g_loss = 0.1 * bce_loss((F.tanh(gen_y)+1)/2, (y_train+1)/2)\n",
    "            g_loss.backward()\n",
    "            g_losses.append(g_loss)\n",
    "            optimizer_G.step()\n",
    "        elif epoch % k == 0:\n",
    "            r_decision = discriminator_R(gen_data)\n",
    "            r_gen = bce_loss(r_decision, generated)\n",
    "            \n",
    "            # ---------------------------------\n",
    "            #  Re-weights using output of D_R\n",
    "            # ---------------------------------\n",
    "            if epoch % 100 == 0:\n",
    "                loss_ratio = (g_losses[-1]/d_r_losses[-1]).detach()\n",
    "                a = 1/(1+torch.exp(-(loss_ratio-3)))\n",
    "                b = 1-a\n",
    "                r_weight_tmp = r_decision.detach().squeeze()\n",
    "                r_weight = a * r_weight_tmp + b * r_ones\n",
    "\n",
    "            f_cost = F.binary_cross_entropy(discriminator_F(gen_y), s1_train, reduction=\"none\").squeeze()\n",
    "            g_cost = F.binary_cross_entropy_with_logits(gen_y.squeeze(), (y_train.squeeze()+1)/2, reduction=\"none\").squeeze()\n",
    "\n",
    "            f_gen = torch.mean(f_cost*r_weight)\n",
    "            g_loss = (1-lambda_f-lambda_r) * torch.mean(g_cost*r_weight) - lambda_f * f_gen -  lambda_r * r_gen \n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "\n",
    "        g_losses.append(g_loss)\n",
    "\n",
    "        if epoch % 200 == 0:\n",
    "            print(\n",
    "                    \"[Lambda: %1f] [Epoch %d/%d] [D_F loss: %f] [D_R loss: %f] [G loss: %f]\"\n",
    "                    % (lambda_f, epoch, n_epochs, d_f_losses[-1], d_r_losses[-1], g_losses[-1])\n",
    "                )\n",
    "\n",
    "#     torch.save(generator.state_dict(), './FR-Train_on_poi_synthetic.pth')\n",
    "    tmp = test_model(generator, XS_test, y_test, s1_test)\n",
    "    test_result.append([lambda_f, lambda_r, tmp[0].item(), tmp[1]])\n",
    "\n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.100000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.100000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.100000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.100000] [Epoch 600/10000] [D_F loss: 0.672426] [D_R loss: 0.689105] [G loss: -0.083195]\n",
      "[Lambda: 0.100000] [Epoch 800/10000] [D_F loss: 0.675748] [D_R loss: 0.686940] [G loss: -0.078719]\n",
      "[Lambda: 0.100000] [Epoch 1000/10000] [D_F loss: 0.674420] [D_R loss: 0.684453] [G loss: -0.074216]\n",
      "[Lambda: 0.100000] [Epoch 1200/10000] [D_F loss: 0.673476] [D_R loss: 0.682800] [G loss: -0.070523]\n",
      "[Lambda: 0.100000] [Epoch 1400/10000] [D_F loss: 0.672513] [D_R loss: 0.681618] [G loss: -0.067542]\n",
      "[Lambda: 0.100000] [Epoch 1600/10000] [D_F loss: 0.671613] [D_R loss: 0.680717] [G loss: -0.065112]\n",
      "[Lambda: 0.100000] [Epoch 1800/10000] [D_F loss: 0.670873] [D_R loss: 0.680004] [G loss: -0.063117]\n",
      "[Lambda: 0.100000] [Epoch 2000/10000] [D_F loss: 0.670276] [D_R loss: 0.679405] [G loss: -0.061455]\n",
      "[Lambda: 0.100000] [Epoch 2200/10000] [D_F loss: 0.669853] [D_R loss: 0.678902] [G loss: -0.060052]\n",
      "[Lambda: 0.100000] [Epoch 2400/10000] [D_F loss: 0.669574] [D_R loss: 0.678461] [G loss: -0.058862]\n",
      "[Lambda: 0.100000] [Epoch 2600/10000] [D_F loss: 0.669364] [D_R loss: 0.678115] [G loss: -0.057835]\n",
      "[Lambda: 0.100000] [Epoch 2800/10000] [D_F loss: 0.669273] [D_R loss: 0.677843] [G loss: -0.056953]\n",
      "[Lambda: 0.100000] [Epoch 3000/10000] [D_F loss: 0.669335] [D_R loss: 0.677596] [G loss: -0.056190]\n",
      "[Lambda: 0.100000] [Epoch 3200/10000] [D_F loss: 0.669414] [D_R loss: 0.677383] [G loss: -0.055521]\n",
      "[Lambda: 0.100000] [Epoch 3400/10000] [D_F loss: 0.669588] [D_R loss: 0.677187] [G loss: -0.054926]\n",
      "[Lambda: 0.100000] [Epoch 3600/10000] [D_F loss: 0.669793] [D_R loss: 0.677032] [G loss: -0.054387]\n",
      "[Lambda: 0.100000] [Epoch 3800/10000] [D_F loss: 0.670030] [D_R loss: 0.676918] [G loss: -0.053906]\n",
      "[Lambda: 0.100000] [Epoch 4000/10000] [D_F loss: 0.670236] [D_R loss: 0.676838] [G loss: -0.053456]\n",
      "[Lambda: 0.100000] [Epoch 4200/10000] [D_F loss: 0.670556] [D_R loss: 0.676770] [G loss: -0.053038]\n",
      "[Lambda: 0.100000] [Epoch 4400/10000] [D_F loss: 0.670920] [D_R loss: 0.676732] [G loss: -0.052650]\n",
      "[Lambda: 0.100000] [Epoch 4600/10000] [D_F loss: 0.671294] [D_R loss: 0.676692] [G loss: -0.052282]\n",
      "[Lambda: 0.100000] [Epoch 4800/10000] [D_F loss: 0.671641] [D_R loss: 0.676659] [G loss: -0.051937]\n",
      "[Lambda: 0.100000] [Epoch 5000/10000] [D_F loss: 0.671943] [D_R loss: 0.676649] [G loss: -0.051611]\n",
      "[Lambda: 0.100000] [Epoch 5200/10000] [D_F loss: 0.672234] [D_R loss: 0.676631] [G loss: -0.051288]\n",
      "[Lambda: 0.100000] [Epoch 5400/10000] [D_F loss: 0.672618] [D_R loss: 0.676616] [G loss: -0.050966]\n",
      "[Lambda: 0.100000] [Epoch 5600/10000] [D_F loss: 0.672901] [D_R loss: 0.676611] [G loss: -0.050643]\n",
      "[Lambda: 0.100000] [Epoch 5800/10000] [D_F loss: 0.673206] [D_R loss: 0.676588] [G loss: -0.050333]\n",
      "[Lambda: 0.100000] [Epoch 6000/10000] [D_F loss: 0.673514] [D_R loss: 0.676559] [G loss: -0.050031]\n",
      "[Lambda: 0.100000] [Epoch 6200/10000] [D_F loss: 0.673664] [D_R loss: 0.676551] [G loss: -0.049731]\n",
      "[Lambda: 0.100000] [Epoch 6400/10000] [D_F loss: 0.673893] [D_R loss: 0.676528] [G loss: -0.049435]\n",
      "[Lambda: 0.100000] [Epoch 6600/10000] [D_F loss: 0.674059] [D_R loss: 0.676484] [G loss: -0.049141]\n",
      "[Lambda: 0.100000] [Epoch 6800/10000] [D_F loss: 0.674233] [D_R loss: 0.676436] [G loss: -0.048849]\n",
      "[Lambda: 0.100000] [Epoch 7000/10000] [D_F loss: 0.674463] [D_R loss: 0.676375] [G loss: -0.048567]\n",
      "[Lambda: 0.100000] [Epoch 7200/10000] [D_F loss: 0.674648] [D_R loss: 0.676281] [G loss: -0.048298]\n",
      "[Lambda: 0.100000] [Epoch 7400/10000] [D_F loss: 0.674695] [D_R loss: 0.676219] [G loss: -0.048022]\n",
      "[Lambda: 0.100000] [Epoch 7600/10000] [D_F loss: 0.674816] [D_R loss: 0.676130] [G loss: -0.047758]\n",
      "[Lambda: 0.100000] [Epoch 7800/10000] [D_F loss: 0.674981] [D_R loss: 0.676059] [G loss: -0.047499]\n",
      "[Lambda: 0.100000] [Epoch 8000/10000] [D_F loss: 0.675037] [D_R loss: 0.675991] [G loss: -0.047235]\n",
      "[Lambda: 0.100000] [Epoch 8200/10000] [D_F loss: 0.675214] [D_R loss: 0.675880] [G loss: -0.046988]\n",
      "[Lambda: 0.100000] [Epoch 8400/10000] [D_F loss: 0.675350] [D_R loss: 0.675752] [G loss: -0.046762]\n",
      "[Lambda: 0.100000] [Epoch 8600/10000] [D_F loss: 0.675401] [D_R loss: 0.675671] [G loss: -0.046545]\n",
      "[Lambda: 0.100000] [Epoch 8800/10000] [D_F loss: 0.675458] [D_R loss: 0.675564] [G loss: -0.046327]\n",
      "[Lambda: 0.100000] [Epoch 9000/10000] [D_F loss: 0.675500] [D_R loss: 0.675446] [G loss: -0.046091]\n",
      "[Lambda: 0.100000] [Epoch 9200/10000] [D_F loss: 0.675511] [D_R loss: 0.675271] [G loss: -0.045872]\n",
      "[Lambda: 0.100000] [Epoch 9400/10000] [D_F loss: 0.675494] [D_R loss: 0.675101] [G loss: -0.045644]\n",
      "[Lambda: 0.100000] [Epoch 9600/10000] [D_F loss: 0.675621] [D_R loss: 0.674867] [G loss: -0.045416]\n",
      "[Lambda: 0.100000] [Epoch 9800/10000] [D_F loss: 0.675697] [D_R loss: 0.674639] [G loss: -0.045196]\n",
      "Test accuracy: 0.8420000076293945\n",
      "P(y_hat=1 | z=0) = 0.403, P(y_hat=1 | z=1) = 0.614\n",
      "P(y_hat=1 | y=1, z=0) = 0.859, P(y_hat=1 | y=1, z=1) = 0.800\n",
      "Disparate Impact ratio = 0.657\n",
      "[Lambda: 0.150000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.150000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.150000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.150000] [Epoch 600/10000] [D_F loss: 0.674199] [D_R loss: 0.689906] [G loss: -0.144668]\n",
      "[Lambda: 0.150000] [Epoch 800/10000] [D_F loss: 0.678243] [D_R loss: 0.687683] [G loss: -0.140153]\n",
      "[Lambda: 0.150000] [Epoch 1000/10000] [D_F loss: 0.677861] [D_R loss: 0.685341] [G loss: -0.135638]\n",
      "[Lambda: 0.150000] [Epoch 1200/10000] [D_F loss: 0.677633] [D_R loss: 0.683627] [G loss: -0.131927]\n",
      "[Lambda: 0.150000] [Epoch 1400/10000] [D_F loss: 0.677350] [D_R loss: 0.682392] [G loss: -0.128946]\n",
      "[Lambda: 0.150000] [Epoch 1600/10000] [D_F loss: 0.677138] [D_R loss: 0.681490] [G loss: -0.126530]\n",
      "[Lambda: 0.150000] [Epoch 1800/10000] [D_F loss: 0.677011] [D_R loss: 0.680795] [G loss: -0.124556]\n",
      "[Lambda: 0.150000] [Epoch 2000/10000] [D_F loss: 0.677015] [D_R loss: 0.680243] [G loss: -0.122922]\n",
      "[Lambda: 0.150000] [Epoch 2200/10000] [D_F loss: 0.677138] [D_R loss: 0.679785] [G loss: -0.121548]\n",
      "[Lambda: 0.150000] [Epoch 2400/10000] [D_F loss: 0.677286] [D_R loss: 0.679406] [G loss: -0.120378]\n",
      "[Lambda: 0.150000] [Epoch 2600/10000] [D_F loss: 0.677505] [D_R loss: 0.679092] [G loss: -0.119375]\n",
      "[Lambda: 0.150000] [Epoch 2800/10000] [D_F loss: 0.677748] [D_R loss: 0.678848] [G loss: -0.118520]\n",
      "[Lambda: 0.150000] [Epoch 3000/10000] [D_F loss: 0.678045] [D_R loss: 0.678647] [G loss: -0.117773]\n",
      "[Lambda: 0.150000] [Epoch 3200/10000] [D_F loss: 0.678447] [D_R loss: 0.678495] [G loss: -0.117114]\n",
      "[Lambda: 0.150000] [Epoch 3400/10000] [D_F loss: 0.678735] [D_R loss: 0.678376] [G loss: -0.116532]\n",
      "[Lambda: 0.150000] [Epoch 3600/10000] [D_F loss: 0.679129] [D_R loss: 0.678279] [G loss: -0.116009]\n",
      "[Lambda: 0.150000] [Epoch 3800/10000] [D_F loss: 0.679484] [D_R loss: 0.678188] [G loss: -0.115532]\n",
      "[Lambda: 0.150000] [Epoch 4000/10000] [D_F loss: 0.679885] [D_R loss: 0.678132] [G loss: -0.115101]\n",
      "[Lambda: 0.150000] [Epoch 4200/10000] [D_F loss: 0.680189] [D_R loss: 0.678121] [G loss: -0.114701]\n",
      "[Lambda: 0.150000] [Epoch 4400/10000] [D_F loss: 0.680467] [D_R loss: 0.678127] [G loss: -0.114314]\n",
      "[Lambda: 0.150000] [Epoch 4600/10000] [D_F loss: 0.680695] [D_R loss: 0.678106] [G loss: -0.113948]\n",
      "[Lambda: 0.150000] [Epoch 4800/10000] [D_F loss: 0.680866] [D_R loss: 0.678116] [G loss: -0.113589]\n",
      "[Lambda: 0.150000] [Epoch 5000/10000] [D_F loss: 0.681103] [D_R loss: 0.678115] [G loss: -0.113246]\n",
      "[Lambda: 0.150000] [Epoch 5200/10000] [D_F loss: 0.681304] [D_R loss: 0.678122] [G loss: -0.112913]\n",
      "[Lambda: 0.150000] [Epoch 5400/10000] [D_F loss: 0.681541] [D_R loss: 0.678104] [G loss: -0.112587]\n",
      "[Lambda: 0.150000] [Epoch 5600/10000] [D_F loss: 0.681712] [D_R loss: 0.678098] [G loss: -0.112275]\n",
      "[Lambda: 0.150000] [Epoch 5800/10000] [D_F loss: 0.681781] [D_R loss: 0.678101] [G loss: -0.111980]\n",
      "[Lambda: 0.150000] [Epoch 6000/10000] [D_F loss: 0.681853] [D_R loss: 0.678096] [G loss: -0.111689]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.150000] [Epoch 6200/10000] [D_F loss: 0.681877] [D_R loss: 0.678068] [G loss: -0.111400]\n",
      "[Lambda: 0.150000] [Epoch 6400/10000] [D_F loss: 0.681847] [D_R loss: 0.678019] [G loss: -0.111119]\n",
      "[Lambda: 0.150000] [Epoch 6600/10000] [D_F loss: 0.681840] [D_R loss: 0.677954] [G loss: -0.110843]\n",
      "[Lambda: 0.150000] [Epoch 6800/10000] [D_F loss: 0.681796] [D_R loss: 0.677864] [G loss: -0.110574]\n",
      "[Lambda: 0.150000] [Epoch 7000/10000] [D_F loss: 0.681815] [D_R loss: 0.677781] [G loss: -0.110301]\n",
      "[Lambda: 0.150000] [Epoch 7200/10000] [D_F loss: 0.681768] [D_R loss: 0.677701] [G loss: -0.110019]\n",
      "[Lambda: 0.150000] [Epoch 7400/10000] [D_F loss: 0.681719] [D_R loss: 0.677602] [G loss: -0.109760]\n",
      "[Lambda: 0.150000] [Epoch 7600/10000] [D_F loss: 0.681623] [D_R loss: 0.677501] [G loss: -0.109508]\n",
      "[Lambda: 0.150000] [Epoch 7800/10000] [D_F loss: 0.681566] [D_R loss: 0.677377] [G loss: -0.109268]\n",
      "[Lambda: 0.150000] [Epoch 8000/10000] [D_F loss: 0.681487] [D_R loss: 0.677279] [G loss: -0.109035]\n",
      "[Lambda: 0.150000] [Epoch 8200/10000] [D_F loss: 0.681396] [D_R loss: 0.677189] [G loss: -0.108815]\n",
      "[Lambda: 0.150000] [Epoch 8400/10000] [D_F loss: 0.681348] [D_R loss: 0.677105] [G loss: -0.108598]\n",
      "[Lambda: 0.150000] [Epoch 8600/10000] [D_F loss: 0.681275] [D_R loss: 0.677023] [G loss: -0.108394]\n",
      "[Lambda: 0.150000] [Epoch 8800/10000] [D_F loss: 0.681174] [D_R loss: 0.676972] [G loss: -0.108186]\n",
      "[Lambda: 0.150000] [Epoch 9000/10000] [D_F loss: 0.681084] [D_R loss: 0.676841] [G loss: -0.107971]\n",
      "[Lambda: 0.150000] [Epoch 9200/10000] [D_F loss: 0.681071] [D_R loss: 0.676677] [G loss: -0.107755]\n",
      "[Lambda: 0.150000] [Epoch 9400/10000] [D_F loss: 0.681098] [D_R loss: 0.676472] [G loss: -0.107563]\n",
      "[Lambda: 0.150000] [Epoch 9600/10000] [D_F loss: 0.681147] [D_R loss: 0.676284] [G loss: -0.107377]\n",
      "[Lambda: 0.150000] [Epoch 9800/10000] [D_F loss: 0.681121] [D_R loss: 0.676092] [G loss: -0.107192]\n",
      "Test accuracy: 0.8349999785423279\n",
      "P(y_hat=1 | z=0) = 0.422, P(y_hat=1 | z=1) = 0.600\n",
      "P(y_hat=1 | y=1, z=0) = 0.879, P(y_hat=1 | y=1, z=1) = 0.784\n",
      "Disparate Impact ratio = 0.704\n",
      "[Lambda: 0.200000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.200000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.200000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.200000] [Epoch 600/10000] [D_F loss: 0.677185] [D_R loss: 0.691261] [G loss: -0.206682]\n",
      "[Lambda: 0.200000] [Epoch 800/10000] [D_F loss: 0.681151] [D_R loss: 0.688625] [G loss: -0.202106]\n",
      "[Lambda: 0.200000] [Epoch 1000/10000] [D_F loss: 0.681440] [D_R loss: 0.686335] [G loss: -0.197640]\n",
      "[Lambda: 0.200000] [Epoch 1200/10000] [D_F loss: 0.681753] [D_R loss: 0.684568] [G loss: -0.193966]\n",
      "[Lambda: 0.200000] [Epoch 1400/10000] [D_F loss: 0.681918] [D_R loss: 0.683278] [G loss: -0.191018]\n",
      "[Lambda: 0.200000] [Epoch 1600/10000] [D_F loss: 0.682115] [D_R loss: 0.682342] [G loss: -0.188641]\n",
      "[Lambda: 0.200000] [Epoch 1800/10000] [D_F loss: 0.682322] [D_R loss: 0.681644] [G loss: -0.186713]\n",
      "[Lambda: 0.200000] [Epoch 2000/10000] [D_F loss: 0.682524] [D_R loss: 0.681108] [G loss: -0.185116]\n",
      "[Lambda: 0.200000] [Epoch 2200/10000] [D_F loss: 0.682820] [D_R loss: 0.680689] [G loss: -0.183775]\n",
      "[Lambda: 0.200000] [Epoch 2400/10000] [D_F loss: 0.683084] [D_R loss: 0.680354] [G loss: -0.182627]\n",
      "[Lambda: 0.200000] [Epoch 2600/10000] [D_F loss: 0.683452] [D_R loss: 0.680100] [G loss: -0.181655]\n",
      "[Lambda: 0.200000] [Epoch 2800/10000] [D_F loss: 0.683764] [D_R loss: 0.679910] [G loss: -0.180818]\n",
      "[Lambda: 0.200000] [Epoch 3000/10000] [D_F loss: 0.684054] [D_R loss: 0.679752] [G loss: -0.180088]\n",
      "[Lambda: 0.200000] [Epoch 3200/10000] [D_F loss: 0.684292] [D_R loss: 0.679634] [G loss: -0.179438]\n",
      "[Lambda: 0.200000] [Epoch 3400/10000] [D_F loss: 0.684572] [D_R loss: 0.679568] [G loss: -0.178861]\n",
      "[Lambda: 0.200000] [Epoch 3600/10000] [D_F loss: 0.684768] [D_R loss: 0.679526] [G loss: -0.178331]\n",
      "[Lambda: 0.200000] [Epoch 3800/10000] [D_F loss: 0.684970] [D_R loss: 0.679497] [G loss: -0.177846]\n",
      "[Lambda: 0.200000] [Epoch 4000/10000] [D_F loss: 0.685126] [D_R loss: 0.679474] [G loss: -0.177397]\n",
      "[Lambda: 0.200000] [Epoch 4200/10000] [D_F loss: 0.685265] [D_R loss: 0.679462] [G loss: -0.176973]\n",
      "[Lambda: 0.200000] [Epoch 4400/10000] [D_F loss: 0.685351] [D_R loss: 0.679498] [G loss: -0.176572]\n",
      "[Lambda: 0.200000] [Epoch 4600/10000] [D_F loss: 0.685418] [D_R loss: 0.679542] [G loss: -0.176185]\n",
      "[Lambda: 0.200000] [Epoch 4800/10000] [D_F loss: 0.685447] [D_R loss: 0.679548] [G loss: -0.175817]\n",
      "[Lambda: 0.200000] [Epoch 5000/10000] [D_F loss: 0.685459] [D_R loss: 0.679543] [G loss: -0.175470]\n",
      "[Lambda: 0.200000] [Epoch 5200/10000] [D_F loss: 0.685487] [D_R loss: 0.679511] [G loss: -0.175137]\n",
      "[Lambda: 0.200000] [Epoch 5400/10000] [D_F loss: 0.685460] [D_R loss: 0.679489] [G loss: -0.174809]\n",
      "[Lambda: 0.200000] [Epoch 5600/10000] [D_F loss: 0.685421] [D_R loss: 0.679474] [G loss: -0.174496]\n",
      "[Lambda: 0.200000] [Epoch 5800/10000] [D_F loss: 0.685346] [D_R loss: 0.679409] [G loss: -0.174201]\n",
      "[Lambda: 0.200000] [Epoch 6000/10000] [D_F loss: 0.685189] [D_R loss: 0.679355] [G loss: -0.173918]\n",
      "[Lambda: 0.200000] [Epoch 6200/10000] [D_F loss: 0.685024] [D_R loss: 0.679310] [G loss: -0.173642]\n",
      "[Lambda: 0.200000] [Epoch 6400/10000] [D_F loss: 0.684908] [D_R loss: 0.679241] [G loss: -0.173377]\n",
      "[Lambda: 0.200000] [Epoch 6600/10000] [D_F loss: 0.684774] [D_R loss: 0.679158] [G loss: -0.173115]\n",
      "[Lambda: 0.200000] [Epoch 6800/10000] [D_F loss: 0.684654] [D_R loss: 0.679084] [G loss: -0.172842]\n",
      "[Lambda: 0.200000] [Epoch 7000/10000] [D_F loss: 0.684546] [D_R loss: 0.679002] [G loss: -0.172581]\n",
      "[Lambda: 0.200000] [Epoch 7200/10000] [D_F loss: 0.684420] [D_R loss: 0.678903] [G loss: -0.172337]\n",
      "[Lambda: 0.200000] [Epoch 7400/10000] [D_F loss: 0.684315] [D_R loss: 0.678790] [G loss: -0.172097]\n",
      "[Lambda: 0.200000] [Epoch 7600/10000] [D_F loss: 0.684185] [D_R loss: 0.678693] [G loss: -0.171860]\n",
      "[Lambda: 0.200000] [Epoch 7800/10000] [D_F loss: 0.684078] [D_R loss: 0.678577] [G loss: -0.171634]\n",
      "[Lambda: 0.200000] [Epoch 8000/10000] [D_F loss: 0.683950] [D_R loss: 0.678446] [G loss: -0.171423]\n",
      "[Lambda: 0.200000] [Epoch 8200/10000] [D_F loss: 0.683871] [D_R loss: 0.678305] [G loss: -0.171220]\n",
      "[Lambda: 0.200000] [Epoch 8400/10000] [D_F loss: 0.683774] [D_R loss: 0.678166] [G loss: -0.171025]\n",
      "[Lambda: 0.200000] [Epoch 8600/10000] [D_F loss: 0.683687] [D_R loss: 0.678042] [G loss: -0.170838]\n",
      "[Lambda: 0.200000] [Epoch 8800/10000] [D_F loss: 0.683588] [D_R loss: 0.677918] [G loss: -0.170650]\n",
      "[Lambda: 0.200000] [Epoch 9000/10000] [D_F loss: 0.683529] [D_R loss: 0.677780] [G loss: -0.170469]\n",
      "[Lambda: 0.200000] [Epoch 9200/10000] [D_F loss: 0.683532] [D_R loss: 0.677561] [G loss: -0.170289]\n",
      "[Lambda: 0.200000] [Epoch 9400/10000] [D_F loss: 0.683517] [D_R loss: 0.677359] [G loss: -0.170117]\n",
      "[Lambda: 0.200000] [Epoch 9600/10000] [D_F loss: 0.683496] [D_R loss: 0.677236] [G loss: -0.169950]\n",
      "[Lambda: 0.200000] [Epoch 9800/10000] [D_F loss: 0.683447] [D_R loss: 0.677089] [G loss: -0.169777]\n",
      "Test accuracy: 0.8330000042915344\n",
      "P(y_hat=1 | z=0) = 0.426, P(y_hat=1 | z=1) = 0.590\n",
      "P(y_hat=1 | y=1, z=0) = 0.884, P(y_hat=1 | y=1, z=1) = 0.775\n",
      "Disparate Impact ratio = 0.722\n",
      "[Lambda: 0.250000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.250000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.250000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.250000] [Epoch 600/10000] [D_F loss: 0.681211] [D_R loss: 0.693081] [G loss: -0.269243]\n",
      "[Lambda: 0.250000] [Epoch 800/10000] [D_F loss: 0.684863] [D_R loss: 0.689951] [G loss: -0.264627]\n",
      "[Lambda: 0.250000] [Epoch 1000/10000] [D_F loss: 0.685506] [D_R loss: 0.687584] [G loss: -0.260242]\n",
      "[Lambda: 0.250000] [Epoch 1200/10000] [D_F loss: 0.686030] [D_R loss: 0.685675] [G loss: -0.256617]\n",
      "[Lambda: 0.250000] [Epoch 1400/10000] [D_F loss: 0.686259] [D_R loss: 0.684273] [G loss: -0.253713]\n",
      "[Lambda: 0.250000] [Epoch 1600/10000] [D_F loss: 0.686422] [D_R loss: 0.683241] [G loss: -0.251375]\n",
      "[Lambda: 0.250000] [Epoch 1800/10000] [D_F loss: 0.686595] [D_R loss: 0.682521] [G loss: -0.249484]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.250000] [Epoch 2000/10000] [D_F loss: 0.686747] [D_R loss: 0.682007] [G loss: -0.247917]\n",
      "[Lambda: 0.250000] [Epoch 2200/10000] [D_F loss: 0.686878] [D_R loss: 0.681598] [G loss: -0.246595]\n",
      "[Lambda: 0.250000] [Epoch 2400/10000] [D_F loss: 0.687032] [D_R loss: 0.681332] [G loss: -0.245469]\n",
      "[Lambda: 0.250000] [Epoch 2600/10000] [D_F loss: 0.687178] [D_R loss: 0.681141] [G loss: -0.244499]\n",
      "[Lambda: 0.250000] [Epoch 2800/10000] [D_F loss: 0.687370] [D_R loss: 0.680988] [G loss: -0.243658]\n",
      "[Lambda: 0.250000] [Epoch 3000/10000] [D_F loss: 0.687492] [D_R loss: 0.680869] [G loss: -0.242916]\n",
      "[Lambda: 0.250000] [Epoch 3200/10000] [D_F loss: 0.687491] [D_R loss: 0.680804] [G loss: -0.242256]\n",
      "[Lambda: 0.250000] [Epoch 3400/10000] [D_F loss: 0.687513] [D_R loss: 0.680760] [G loss: -0.241658]\n",
      "[Lambda: 0.250000] [Epoch 3600/10000] [D_F loss: 0.687549] [D_R loss: 0.680739] [G loss: -0.241106]\n",
      "[Lambda: 0.250000] [Epoch 3800/10000] [D_F loss: 0.687515] [D_R loss: 0.680758] [G loss: -0.240610]\n",
      "[Lambda: 0.250000] [Epoch 4000/10000] [D_F loss: 0.687501] [D_R loss: 0.680793] [G loss: -0.240145]\n",
      "[Lambda: 0.250000] [Epoch 4200/10000] [D_F loss: 0.687471] [D_R loss: 0.680800] [G loss: -0.239700]\n",
      "[Lambda: 0.250000] [Epoch 4400/10000] [D_F loss: 0.687382] [D_R loss: 0.680791] [G loss: -0.239281]\n",
      "[Lambda: 0.250000] [Epoch 4600/10000] [D_F loss: 0.687275] [D_R loss: 0.680784] [G loss: -0.238876]\n",
      "[Lambda: 0.250000] [Epoch 4800/10000] [D_F loss: 0.687156] [D_R loss: 0.680770] [G loss: -0.238493]\n",
      "[Lambda: 0.250000] [Epoch 5000/10000] [D_F loss: 0.687055] [D_R loss: 0.680754] [G loss: -0.238138]\n",
      "[Lambda: 0.250000] [Epoch 5200/10000] [D_F loss: 0.686968] [D_R loss: 0.680722] [G loss: -0.237790]\n",
      "[Lambda: 0.250000] [Epoch 5400/10000] [D_F loss: 0.686780] [D_R loss: 0.680678] [G loss: -0.237458]\n",
      "[Lambda: 0.250000] [Epoch 5600/10000] [D_F loss: 0.686662] [D_R loss: 0.680642] [G loss: -0.237143]\n",
      "[Lambda: 0.250000] [Epoch 5800/10000] [D_F loss: 0.686462] [D_R loss: 0.680591] [G loss: -0.236842]\n",
      "[Lambda: 0.250000] [Epoch 6000/10000] [D_F loss: 0.686305] [D_R loss: 0.680508] [G loss: -0.236547]\n",
      "[Lambda: 0.250000] [Epoch 6200/10000] [D_F loss: 0.686177] [D_R loss: 0.680404] [G loss: -0.236262]\n",
      "[Lambda: 0.250000] [Epoch 6400/10000] [D_F loss: 0.686056] [D_R loss: 0.680291] [G loss: -0.235996]\n",
      "[Lambda: 0.250000] [Epoch 6600/10000] [D_F loss: 0.685883] [D_R loss: 0.680202] [G loss: -0.235730]\n",
      "[Lambda: 0.250000] [Epoch 6800/10000] [D_F loss: 0.685722] [D_R loss: 0.680084] [G loss: -0.235469]\n",
      "[Lambda: 0.250000] [Epoch 7000/10000] [D_F loss: 0.685611] [D_R loss: 0.679946] [G loss: -0.235230]\n",
      "[Lambda: 0.250000] [Epoch 7200/10000] [D_F loss: 0.685501] [D_R loss: 0.679809] [G loss: -0.235005]\n",
      "[Lambda: 0.250000] [Epoch 7400/10000] [D_F loss: 0.685384] [D_R loss: 0.679674] [G loss: -0.234785]\n",
      "[Lambda: 0.250000] [Epoch 7600/10000] [D_F loss: 0.685286] [D_R loss: 0.679534] [G loss: -0.234582]\n",
      "[Lambda: 0.250000] [Epoch 7800/10000] [D_F loss: 0.685185] [D_R loss: 0.679379] [G loss: -0.234386]\n",
      "[Lambda: 0.250000] [Epoch 8000/10000] [D_F loss: 0.685097] [D_R loss: 0.679231] [G loss: -0.234202]\n",
      "[Lambda: 0.250000] [Epoch 8200/10000] [D_F loss: 0.685021] [D_R loss: 0.679095] [G loss: -0.234024]\n",
      "[Lambda: 0.250000] [Epoch 8400/10000] [D_F loss: 0.684957] [D_R loss: 0.678955] [G loss: -0.233852]\n",
      "[Lambda: 0.250000] [Epoch 8600/10000] [D_F loss: 0.684874] [D_R loss: 0.678847] [G loss: -0.233696]\n",
      "[Lambda: 0.250000] [Epoch 8800/10000] [D_F loss: 0.684838] [D_R loss: 0.678716] [G loss: -0.233542]\n",
      "[Lambda: 0.250000] [Epoch 9000/10000] [D_F loss: 0.684836] [D_R loss: 0.678548] [G loss: -0.233386]\n",
      "[Lambda: 0.250000] [Epoch 9200/10000] [D_F loss: 0.684886] [D_R loss: 0.678367] [G loss: -0.233231]\n",
      "[Lambda: 0.250000] [Epoch 9400/10000] [D_F loss: 0.684880] [D_R loss: 0.678195] [G loss: -0.233078]\n",
      "[Lambda: 0.250000] [Epoch 9600/10000] [D_F loss: 0.684855] [D_R loss: 0.678001] [G loss: -0.232934]\n",
      "[Lambda: 0.250000] [Epoch 9800/10000] [D_F loss: 0.684890] [D_R loss: 0.677783] [G loss: -0.232791]\n",
      "Test accuracy: 0.8270000219345093\n",
      "P(y_hat=1 | z=0) = 0.435, P(y_hat=1 | z=1) = 0.583\n",
      "P(y_hat=1 | y=1, z=0) = 0.889, P(y_hat=1 | y=1, z=1) = 0.766\n",
      "Disparate Impact ratio = 0.745\n",
      "[Lambda: 0.300000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.300000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.300000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.300000] [Epoch 600/10000] [D_F loss: 0.684667] [D_R loss: 0.694771] [G loss: -0.332406]\n",
      "[Lambda: 0.300000] [Epoch 800/10000] [D_F loss: 0.689865] [D_R loss: 0.691798] [G loss: -0.327831]\n",
      "[Lambda: 0.300000] [Epoch 1000/10000] [D_F loss: 0.689875] [D_R loss: 0.689065] [G loss: -0.323487]\n",
      "[Lambda: 0.300000] [Epoch 1200/10000] [D_F loss: 0.690117] [D_R loss: 0.687003] [G loss: -0.319881]\n",
      "[Lambda: 0.300000] [Epoch 1400/10000] [D_F loss: 0.690029] [D_R loss: 0.685424] [G loss: -0.316977]\n",
      "[Lambda: 0.300000] [Epoch 1600/10000] [D_F loss: 0.689964] [D_R loss: 0.684271] [G loss: -0.314644]\n",
      "[Lambda: 0.300000] [Epoch 1800/10000] [D_F loss: 0.689756] [D_R loss: 0.683488] [G loss: -0.312754]\n",
      "[Lambda: 0.300000] [Epoch 2000/10000] [D_F loss: 0.689525] [D_R loss: 0.682912] [G loss: -0.311178]\n",
      "[Lambda: 0.300000] [Epoch 2200/10000] [D_F loss: 0.689449] [D_R loss: 0.682563] [G loss: -0.309851]\n",
      "[Lambda: 0.300000] [Epoch 2400/10000] [D_F loss: 0.689248] [D_R loss: 0.682308] [G loss: -0.308715]\n",
      "[Lambda: 0.300000] [Epoch 2600/10000] [D_F loss: 0.689160] [D_R loss: 0.682158] [G loss: -0.307725]\n",
      "[Lambda: 0.300000] [Epoch 2800/10000] [D_F loss: 0.689051] [D_R loss: 0.682060] [G loss: -0.306859]\n",
      "[Lambda: 0.300000] [Epoch 3000/10000] [D_F loss: 0.688865] [D_R loss: 0.681977] [G loss: -0.306092]\n",
      "[Lambda: 0.300000] [Epoch 3200/10000] [D_F loss: 0.688732] [D_R loss: 0.681930] [G loss: -0.305394]\n",
      "[Lambda: 0.300000] [Epoch 3400/10000] [D_F loss: 0.688550] [D_R loss: 0.681943] [G loss: -0.304770]\n",
      "[Lambda: 0.300000] [Epoch 3600/10000] [D_F loss: 0.688415] [D_R loss: 0.681973] [G loss: -0.304196]\n",
      "[Lambda: 0.300000] [Epoch 3800/10000] [D_F loss: 0.688290] [D_R loss: 0.681972] [G loss: -0.303664]\n",
      "[Lambda: 0.300000] [Epoch 4000/10000] [D_F loss: 0.688119] [D_R loss: 0.681946] [G loss: -0.303176]\n",
      "[Lambda: 0.300000] [Epoch 4200/10000] [D_F loss: 0.687940] [D_R loss: 0.681925] [G loss: -0.302718]\n",
      "[Lambda: 0.300000] [Epoch 4400/10000] [D_F loss: 0.687804] [D_R loss: 0.681923] [G loss: -0.302291]\n",
      "[Lambda: 0.300000] [Epoch 4600/10000] [D_F loss: 0.687685] [D_R loss: 0.681893] [G loss: -0.301883]\n",
      "[Lambda: 0.300000] [Epoch 4800/10000] [D_F loss: 0.687538] [D_R loss: 0.681861] [G loss: -0.301487]\n",
      "[Lambda: 0.300000] [Epoch 5000/10000] [D_F loss: 0.687321] [D_R loss: 0.681834] [G loss: -0.301113]\n",
      "[Lambda: 0.300000] [Epoch 5200/10000] [D_F loss: 0.687196] [D_R loss: 0.681788] [G loss: -0.300760]\n",
      "[Lambda: 0.300000] [Epoch 5400/10000] [D_F loss: 0.687065] [D_R loss: 0.681703] [G loss: -0.300429]\n",
      "[Lambda: 0.300000] [Epoch 5600/10000] [D_F loss: 0.686932] [D_R loss: 0.681599] [G loss: -0.300115]\n",
      "[Lambda: 0.300000] [Epoch 5800/10000] [D_F loss: 0.686797] [D_R loss: 0.681488] [G loss: -0.299818]\n",
      "[Lambda: 0.300000] [Epoch 6000/10000] [D_F loss: 0.686660] [D_R loss: 0.681365] [G loss: -0.299544]\n",
      "[Lambda: 0.300000] [Epoch 6200/10000] [D_F loss: 0.686553] [D_R loss: 0.681235] [G loss: -0.299285]\n",
      "[Lambda: 0.300000] [Epoch 6400/10000] [D_F loss: 0.686434] [D_R loss: 0.681099] [G loss: -0.299037]\n",
      "[Lambda: 0.300000] [Epoch 6600/10000] [D_F loss: 0.686350] [D_R loss: 0.680956] [G loss: -0.298804]\n",
      "[Lambda: 0.300000] [Epoch 6800/10000] [D_F loss: 0.686244] [D_R loss: 0.680824] [G loss: -0.298578]\n",
      "[Lambda: 0.300000] [Epoch 7000/10000] [D_F loss: 0.686137] [D_R loss: 0.680714] [G loss: -0.298358]\n",
      "[Lambda: 0.300000] [Epoch 7200/10000] [D_F loss: 0.686066] [D_R loss: 0.680588] [G loss: -0.298144]\n",
      "[Lambda: 0.300000] [Epoch 7400/10000] [D_F loss: 0.685991] [D_R loss: 0.680437] [G loss: -0.297951]\n",
      "[Lambda: 0.300000] [Epoch 7600/10000] [D_F loss: 0.685936] [D_R loss: 0.680269] [G loss: -0.297768]\n",
      "[Lambda: 0.300000] [Epoch 7800/10000] [D_F loss: 0.685867] [D_R loss: 0.680101] [G loss: -0.297593]\n",
      "[Lambda: 0.300000] [Epoch 8000/10000] [D_F loss: 0.685794] [D_R loss: 0.679947] [G loss: -0.297423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.300000] [Epoch 8200/10000] [D_F loss: 0.685752] [D_R loss: 0.679776] [G loss: -0.297264]\n",
      "[Lambda: 0.300000] [Epoch 8400/10000] [D_F loss: 0.685732] [D_R loss: 0.679572] [G loss: -0.297119]\n",
      "[Lambda: 0.300000] [Epoch 8600/10000] [D_F loss: 0.685692] [D_R loss: 0.679404] [G loss: -0.296986]\n",
      "[Lambda: 0.300000] [Epoch 8800/10000] [D_F loss: 0.685666] [D_R loss: 0.679241] [G loss: -0.296862]\n",
      "[Lambda: 0.300000] [Epoch 9000/10000] [D_F loss: 0.685651] [D_R loss: 0.679053] [G loss: -0.296743]\n",
      "[Lambda: 0.300000] [Epoch 9200/10000] [D_F loss: 0.685660] [D_R loss: 0.678864] [G loss: -0.296615]\n",
      "[Lambda: 0.300000] [Epoch 9400/10000] [D_F loss: 0.685609] [D_R loss: 0.678713] [G loss: -0.296476]\n",
      "[Lambda: 0.300000] [Epoch 9600/10000] [D_F loss: 0.685592] [D_R loss: 0.678545] [G loss: -0.296343]\n",
      "[Lambda: 0.300000] [Epoch 9800/10000] [D_F loss: 0.685596] [D_R loss: 0.678349] [G loss: -0.296204]\n",
      "Test accuracy: 0.8240000009536743\n",
      "P(y_hat=1 | z=0) = 0.436, P(y_hat=1 | z=1) = 0.574\n",
      "P(y_hat=1 | y=1, z=0) = 0.889, P(y_hat=1 | y=1, z=1) = 0.756\n",
      "Disparate Impact ratio = 0.760\n",
      "[Lambda: 0.350000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.350000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.350000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.350000] [Epoch 600/10000] [D_F loss: 0.686400] [D_R loss: 0.695977] [G loss: -0.396142]\n",
      "[Lambda: 0.350000] [Epoch 800/10000] [D_F loss: 0.695986] [D_R loss: 0.694438] [G loss: -0.391995]\n",
      "[Lambda: 0.350000] [Epoch 1000/10000] [D_F loss: 0.695710] [D_R loss: 0.691264] [G loss: -0.387448]\n",
      "[Lambda: 0.350000] [Epoch 1200/10000] [D_F loss: 0.694410] [D_R loss: 0.688679] [G loss: -0.383719]\n",
      "[Lambda: 0.350000] [Epoch 1400/10000] [D_F loss: 0.693323] [D_R loss: 0.686766] [G loss: -0.380719]\n",
      "[Lambda: 0.350000] [Epoch 1600/10000] [D_F loss: 0.692450] [D_R loss: 0.685435] [G loss: -0.378311]\n",
      "[Lambda: 0.350000] [Epoch 1800/10000] [D_F loss: 0.691623] [D_R loss: 0.684495] [G loss: -0.376365]\n",
      "[Lambda: 0.350000] [Epoch 2000/10000] [D_F loss: 0.690900] [D_R loss: 0.683886] [G loss: -0.374761]\n",
      "[Lambda: 0.350000] [Epoch 2200/10000] [D_F loss: 0.690354] [D_R loss: 0.683564] [G loss: -0.373399]\n",
      "[Lambda: 0.350000] [Epoch 2400/10000] [D_F loss: 0.689908] [D_R loss: 0.683335] [G loss: -0.372223]\n",
      "[Lambda: 0.350000] [Epoch 2600/10000] [D_F loss: 0.689543] [D_R loss: 0.683224] [G loss: -0.371196]\n",
      "[Lambda: 0.350000] [Epoch 2800/10000] [D_F loss: 0.689194] [D_R loss: 0.683157] [G loss: -0.370291]\n",
      "[Lambda: 0.350000] [Epoch 3000/10000] [D_F loss: 0.688887] [D_R loss: 0.683119] [G loss: -0.369484]\n",
      "[Lambda: 0.350000] [Epoch 3200/10000] [D_F loss: 0.688635] [D_R loss: 0.683150] [G loss: -0.368761]\n",
      "[Lambda: 0.350000] [Epoch 3400/10000] [D_F loss: 0.688410] [D_R loss: 0.683159] [G loss: -0.368111]\n",
      "[Lambda: 0.350000] [Epoch 3600/10000] [D_F loss: 0.688183] [D_R loss: 0.683169] [G loss: -0.367520]\n",
      "[Lambda: 0.350000] [Epoch 3800/10000] [D_F loss: 0.688021] [D_R loss: 0.683142] [G loss: -0.366963]\n",
      "[Lambda: 0.350000] [Epoch 4000/10000] [D_F loss: 0.687857] [D_R loss: 0.683105] [G loss: -0.366456]\n",
      "[Lambda: 0.350000] [Epoch 4200/10000] [D_F loss: 0.687707] [D_R loss: 0.683062] [G loss: -0.365994]\n",
      "[Lambda: 0.350000] [Epoch 4400/10000] [D_F loss: 0.687531] [D_R loss: 0.683012] [G loss: -0.365564]\n",
      "[Lambda: 0.350000] [Epoch 4600/10000] [D_F loss: 0.687413] [D_R loss: 0.682948] [G loss: -0.365159]\n",
      "[Lambda: 0.350000] [Epoch 4800/10000] [D_F loss: 0.687325] [D_R loss: 0.682850] [G loss: -0.364775]\n",
      "[Lambda: 0.350000] [Epoch 5000/10000] [D_F loss: 0.687218] [D_R loss: 0.682721] [G loss: -0.364422]\n",
      "[Lambda: 0.350000] [Epoch 5200/10000] [D_F loss: 0.687129] [D_R loss: 0.682578] [G loss: -0.364098]\n",
      "[Lambda: 0.350000] [Epoch 5400/10000] [D_F loss: 0.687046] [D_R loss: 0.682420] [G loss: -0.363793]\n",
      "[Lambda: 0.350000] [Epoch 5600/10000] [D_F loss: 0.686969] [D_R loss: 0.682272] [G loss: -0.363497]\n",
      "[Lambda: 0.350000] [Epoch 5800/10000] [D_F loss: 0.686863] [D_R loss: 0.682113] [G loss: -0.363219]\n",
      "[Lambda: 0.350000] [Epoch 6000/10000] [D_F loss: 0.686762] [D_R loss: 0.681969] [G loss: -0.362966]\n",
      "[Lambda: 0.350000] [Epoch 6200/10000] [D_F loss: 0.686705] [D_R loss: 0.681799] [G loss: -0.362722]\n",
      "[Lambda: 0.350000] [Epoch 6400/10000] [D_F loss: 0.686639] [D_R loss: 0.681655] [G loss: -0.362492]\n",
      "[Lambda: 0.350000] [Epoch 6600/10000] [D_F loss: 0.686575] [D_R loss: 0.681515] [G loss: -0.362277]\n",
      "[Lambda: 0.350000] [Epoch 6800/10000] [D_F loss: 0.686524] [D_R loss: 0.681368] [G loss: -0.362066]\n",
      "[Lambda: 0.350000] [Epoch 7000/10000] [D_F loss: 0.686468] [D_R loss: 0.681216] [G loss: -0.361862]\n",
      "[Lambda: 0.350000] [Epoch 7200/10000] [D_F loss: 0.686414] [D_R loss: 0.681055] [G loss: -0.361663]\n",
      "[Lambda: 0.350000] [Epoch 7400/10000] [D_F loss: 0.686385] [D_R loss: 0.680879] [G loss: -0.361483]\n",
      "[Lambda: 0.350000] [Epoch 7600/10000] [D_F loss: 0.686322] [D_R loss: 0.680718] [G loss: -0.361319]\n",
      "[Lambda: 0.350000] [Epoch 7800/10000] [D_F loss: 0.686278] [D_R loss: 0.680555] [G loss: -0.361164]\n",
      "[Lambda: 0.350000] [Epoch 8000/10000] [D_F loss: 0.686227] [D_R loss: 0.680375] [G loss: -0.361016]\n",
      "[Lambda: 0.350000] [Epoch 8200/10000] [D_F loss: 0.686212] [D_R loss: 0.680204] [G loss: -0.360878]\n",
      "[Lambda: 0.350000] [Epoch 8400/10000] [D_F loss: 0.686184] [D_R loss: 0.680035] [G loss: -0.360751]\n",
      "[Lambda: 0.350000] [Epoch 8600/10000] [D_F loss: 0.686152] [D_R loss: 0.679881] [G loss: -0.360629]\n",
      "[Lambda: 0.350000] [Epoch 8800/10000] [D_F loss: 0.686125] [D_R loss: 0.679713] [G loss: -0.360514]\n",
      "[Lambda: 0.350000] [Epoch 9000/10000] [D_F loss: 0.686097] [D_R loss: 0.679539] [G loss: -0.360408]\n",
      "[Lambda: 0.350000] [Epoch 9200/10000] [D_F loss: 0.686086] [D_R loss: 0.679339] [G loss: -0.360288]\n",
      "[Lambda: 0.350000] [Epoch 9400/10000] [D_F loss: 0.686053] [D_R loss: 0.679150] [G loss: -0.360171]\n",
      "[Lambda: 0.350000] [Epoch 9600/10000] [D_F loss: 0.686021] [D_R loss: 0.678986] [G loss: -0.360062]\n",
      "[Lambda: 0.350000] [Epoch 9800/10000] [D_F loss: 0.686003] [D_R loss: 0.678837] [G loss: -0.359951]\n",
      "Test accuracy: 0.8209999799728394\n",
      "P(y_hat=1 | z=0) = 0.435, P(y_hat=1 | z=1) = 0.569\n",
      "P(y_hat=1 | y=1, z=0) = 0.884, P(y_hat=1 | y=1, z=1) = 0.750\n",
      "Disparate Impact ratio = 0.764\n",
      "[Lambda: 0.400000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.400000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.400000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.400000] [Epoch 600/10000] [D_F loss: 0.687351] [D_R loss: 0.697531] [G loss: -0.460282]\n",
      "[Lambda: 0.400000] [Epoch 800/10000] [D_F loss: 0.700627] [D_R loss: 0.697104] [G loss: -0.457660]\n",
      "[Lambda: 0.400000] [Epoch 1000/10000] [D_F loss: 0.702994] [D_R loss: 0.694611] [G loss: -0.452595]\n",
      "[Lambda: 0.400000] [Epoch 1200/10000] [D_F loss: 0.700493] [D_R loss: 0.691476] [G loss: -0.448049]\n",
      "[Lambda: 0.400000] [Epoch 1400/10000] [D_F loss: 0.697025] [D_R loss: 0.688699] [G loss: -0.444597]\n",
      "[Lambda: 0.400000] [Epoch 1600/10000] [D_F loss: 0.694018] [D_R loss: 0.686729] [G loss: -0.442077]\n",
      "[Lambda: 0.400000] [Epoch 1800/10000] [D_F loss: 0.691940] [D_R loss: 0.685443] [G loss: -0.440120]\n",
      "[Lambda: 0.400000] [Epoch 2000/10000] [D_F loss: 0.690551] [D_R loss: 0.684789] [G loss: -0.438524]\n",
      "[Lambda: 0.400000] [Epoch 2200/10000] [D_F loss: 0.689641] [D_R loss: 0.684443] [G loss: -0.437148]\n",
      "[Lambda: 0.400000] [Epoch 2400/10000] [D_F loss: 0.689040] [D_R loss: 0.684354] [G loss: -0.435948]\n",
      "[Lambda: 0.400000] [Epoch 2600/10000] [D_F loss: 0.688576] [D_R loss: 0.684349] [G loss: -0.434886]\n",
      "[Lambda: 0.400000] [Epoch 2800/10000] [D_F loss: 0.688227] [D_R loss: 0.684404] [G loss: -0.433924]\n",
      "[Lambda: 0.400000] [Epoch 3000/10000] [D_F loss: 0.687966] [D_R loss: 0.684441] [G loss: -0.433059]\n",
      "[Lambda: 0.400000] [Epoch 3200/10000] [D_F loss: 0.687775] [D_R loss: 0.684444] [G loss: -0.432281]\n",
      "[Lambda: 0.400000] [Epoch 3400/10000] [D_F loss: 0.687620] [D_R loss: 0.684404] [G loss: -0.431584]\n",
      "[Lambda: 0.400000] [Epoch 3600/10000] [D_F loss: 0.687501] [D_R loss: 0.684367] [G loss: -0.430959]\n",
      "[Lambda: 0.400000] [Epoch 3800/10000] [D_F loss: 0.687410] [D_R loss: 0.684270] [G loss: -0.430394]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.400000] [Epoch 4000/10000] [D_F loss: 0.687337] [D_R loss: 0.684144] [G loss: -0.429876]\n",
      "[Lambda: 0.400000] [Epoch 4200/10000] [D_F loss: 0.687282] [D_R loss: 0.684001] [G loss: -0.429401]\n",
      "[Lambda: 0.400000] [Epoch 4400/10000] [D_F loss: 0.687213] [D_R loss: 0.683840] [G loss: -0.428987]\n",
      "[Lambda: 0.400000] [Epoch 4600/10000] [D_F loss: 0.687148] [D_R loss: 0.683651] [G loss: -0.428610]\n",
      "[Lambda: 0.400000] [Epoch 4800/10000] [D_F loss: 0.687099] [D_R loss: 0.683427] [G loss: -0.428265]\n",
      "[Lambda: 0.400000] [Epoch 5000/10000] [D_F loss: 0.687062] [D_R loss: 0.683200] [G loss: -0.427946]\n",
      "[Lambda: 0.400000] [Epoch 5200/10000] [D_F loss: 0.687020] [D_R loss: 0.682970] [G loss: -0.427649]\n",
      "[Lambda: 0.400000] [Epoch 5400/10000] [D_F loss: 0.686962] [D_R loss: 0.682769] [G loss: -0.427381]\n",
      "[Lambda: 0.400000] [Epoch 5600/10000] [D_F loss: 0.686901] [D_R loss: 0.682546] [G loss: -0.427139]\n",
      "[Lambda: 0.400000] [Epoch 5800/10000] [D_F loss: 0.686862] [D_R loss: 0.682354] [G loss: -0.426919]\n",
      "[Lambda: 0.400000] [Epoch 6000/10000] [D_F loss: 0.686807] [D_R loss: 0.682134] [G loss: -0.426716]\n",
      "[Lambda: 0.400000] [Epoch 6200/10000] [D_F loss: 0.686778] [D_R loss: 0.681936] [G loss: -0.426519]\n",
      "[Lambda: 0.400000] [Epoch 6400/10000] [D_F loss: 0.686749] [D_R loss: 0.681747] [G loss: -0.426330]\n",
      "[Lambda: 0.400000] [Epoch 6600/10000] [D_F loss: 0.686720] [D_R loss: 0.681569] [G loss: -0.426155]\n",
      "[Lambda: 0.400000] [Epoch 6800/10000] [D_F loss: 0.686683] [D_R loss: 0.681395] [G loss: -0.425978]\n",
      "[Lambda: 0.400000] [Epoch 7000/10000] [D_F loss: 0.686642] [D_R loss: 0.681242] [G loss: -0.425805]\n",
      "[Lambda: 0.400000] [Epoch 7200/10000] [D_F loss: 0.686604] [D_R loss: 0.681093] [G loss: -0.425635]\n",
      "[Lambda: 0.400000] [Epoch 7400/10000] [D_F loss: 0.686558] [D_R loss: 0.680966] [G loss: -0.425476]\n",
      "[Lambda: 0.400000] [Epoch 7600/10000] [D_F loss: 0.686514] [D_R loss: 0.680843] [G loss: -0.425325]\n",
      "[Lambda: 0.400000] [Epoch 7800/10000] [D_F loss: 0.686483] [D_R loss: 0.680715] [G loss: -0.425172]\n",
      "[Lambda: 0.400000] [Epoch 8000/10000] [D_F loss: 0.686461] [D_R loss: 0.680607] [G loss: -0.425020]\n",
      "[Lambda: 0.400000] [Epoch 8200/10000] [D_F loss: 0.686411] [D_R loss: 0.680463] [G loss: -0.424882]\n",
      "[Lambda: 0.400000] [Epoch 8400/10000] [D_F loss: 0.686389] [D_R loss: 0.680316] [G loss: -0.424746]\n",
      "[Lambda: 0.400000] [Epoch 8600/10000] [D_F loss: 0.686359] [D_R loss: 0.680178] [G loss: -0.424622]\n",
      "[Lambda: 0.400000] [Epoch 8800/10000] [D_F loss: 0.686332] [D_R loss: 0.680034] [G loss: -0.424510]\n",
      "[Lambda: 0.400000] [Epoch 9000/10000] [D_F loss: 0.686323] [D_R loss: 0.679886] [G loss: -0.424402]\n",
      "[Lambda: 0.400000] [Epoch 9200/10000] [D_F loss: 0.686316] [D_R loss: 0.679729] [G loss: -0.424265]\n",
      "[Lambda: 0.400000] [Epoch 9400/10000] [D_F loss: 0.686280] [D_R loss: 0.679556] [G loss: -0.424144]\n",
      "[Lambda: 0.400000] [Epoch 9600/10000] [D_F loss: 0.686251] [D_R loss: 0.679449] [G loss: -0.424051]\n",
      "[Lambda: 0.400000] [Epoch 9800/10000] [D_F loss: 0.686229] [D_R loss: 0.679316] [G loss: -0.423973]\n",
      "Test accuracy: 0.8209999799728394\n",
      "P(y_hat=1 | z=0) = 0.431, P(y_hat=1 | z=1) = 0.560\n",
      "P(y_hat=1 | y=1, z=0) = 0.884, P(y_hat=1 | y=1, z=1) = 0.741\n",
      "Disparate Impact ratio = 0.770\n",
      "[Lambda: 0.450000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.450000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.450000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.450000] [Epoch 600/10000] [D_F loss: 0.688335] [D_R loss: 0.700169] [G loss: -0.524983]\n",
      "[Lambda: 0.450000] [Epoch 800/10000] [D_F loss: 0.703393] [D_R loss: 0.699997] [G loss: -0.524727]\n",
      "[Lambda: 0.450000] [Epoch 1000/10000] [D_F loss: 0.708182] [D_R loss: 0.698304] [G loss: -0.520059]\n",
      "[Lambda: 0.450000] [Epoch 1200/10000] [D_F loss: 0.705695] [D_R loss: 0.695291] [G loss: -0.513950]\n",
      "[Lambda: 0.450000] [Epoch 1400/10000] [D_F loss: 0.700187] [D_R loss: 0.691890] [G loss: -0.508584]\n",
      "[Lambda: 0.450000] [Epoch 1600/10000] [D_F loss: 0.695023] [D_R loss: 0.688836] [G loss: -0.505025]\n",
      "[Lambda: 0.450000] [Epoch 1800/10000] [D_F loss: 0.691622] [D_R loss: 0.686608] [G loss: -0.503146]\n",
      "[Lambda: 0.450000] [Epoch 2000/10000] [D_F loss: 0.689838] [D_R loss: 0.685508] [G loss: -0.502177]\n",
      "[Lambda: 0.450000] [Epoch 2200/10000] [D_F loss: 0.688904] [D_R loss: 0.685224] [G loss: -0.501345]\n",
      "[Lambda: 0.450000] [Epoch 2400/10000] [D_F loss: 0.688203] [D_R loss: 0.685286] [G loss: -0.500332]\n",
      "[Lambda: 0.450000] [Epoch 2600/10000] [D_F loss: 0.687532] [D_R loss: 0.685566] [G loss: -0.499163]\n",
      "[Lambda: 0.450000] [Epoch 2800/10000] [D_F loss: 0.686917] [D_R loss: 0.685802] [G loss: -0.497942]\n",
      "[Lambda: 0.450000] [Epoch 3000/10000] [D_F loss: 0.686486] [D_R loss: 0.685983] [G loss: -0.496789]\n",
      "[Lambda: 0.450000] [Epoch 3200/10000] [D_F loss: 0.686303] [D_R loss: 0.686008] [G loss: -0.495785]\n",
      "[Lambda: 0.450000] [Epoch 3400/10000] [D_F loss: 0.686362] [D_R loss: 0.685915] [G loss: -0.494948]\n",
      "[Lambda: 0.450000] [Epoch 3600/10000] [D_F loss: 0.686567] [D_R loss: 0.685720] [G loss: -0.494258]\n",
      "[Lambda: 0.450000] [Epoch 3800/10000] [D_F loss: 0.686811] [D_R loss: 0.685411] [G loss: -0.493688]\n",
      "[Lambda: 0.450000] [Epoch 4000/10000] [D_F loss: 0.687042] [D_R loss: 0.685098] [G loss: -0.493213]\n",
      "[Lambda: 0.450000] [Epoch 4200/10000] [D_F loss: 0.687199] [D_R loss: 0.684709] [G loss: -0.492815]\n",
      "[Lambda: 0.450000] [Epoch 4400/10000] [D_F loss: 0.687263] [D_R loss: 0.684293] [G loss: -0.492475]\n",
      "[Lambda: 0.450000] [Epoch 4600/10000] [D_F loss: 0.687253] [D_R loss: 0.683865] [G loss: -0.492179]\n",
      "[Lambda: 0.450000] [Epoch 4800/10000] [D_F loss: 0.687197] [D_R loss: 0.683464] [G loss: -0.491914]\n",
      "[Lambda: 0.450000] [Epoch 5000/10000] [D_F loss: 0.687115] [D_R loss: 0.683088] [G loss: -0.491677]\n",
      "[Lambda: 0.450000] [Epoch 5200/10000] [D_F loss: 0.687031] [D_R loss: 0.682752] [G loss: -0.491454]\n",
      "[Lambda: 0.450000] [Epoch 5400/10000] [D_F loss: 0.686938] [D_R loss: 0.682440] [G loss: -0.491250]\n",
      "[Lambda: 0.450000] [Epoch 5600/10000] [D_F loss: 0.686861] [D_R loss: 0.682140] [G loss: -0.491060]\n",
      "[Lambda: 0.450000] [Epoch 5800/10000] [D_F loss: 0.686795] [D_R loss: 0.681858] [G loss: -0.490881]\n",
      "[Lambda: 0.450000] [Epoch 6000/10000] [D_F loss: 0.686742] [D_R loss: 0.681627] [G loss: -0.490730]\n",
      "[Lambda: 0.450000] [Epoch 6200/10000] [D_F loss: 0.686692] [D_R loss: 0.681392] [G loss: -0.490595]\n",
      "[Lambda: 0.450000] [Epoch 6400/10000] [D_F loss: 0.686662] [D_R loss: 0.681215] [G loss: -0.490471]\n",
      "[Lambda: 0.450000] [Epoch 6600/10000] [D_F loss: 0.686626] [D_R loss: 0.681029] [G loss: -0.490349]\n",
      "[Lambda: 0.450000] [Epoch 6800/10000] [D_F loss: 0.686606] [D_R loss: 0.680875] [G loss: -0.490225]\n",
      "[Lambda: 0.450000] [Epoch 7000/10000] [D_F loss: 0.686588] [D_R loss: 0.680743] [G loss: -0.490106]\n",
      "[Lambda: 0.450000] [Epoch 7200/10000] [D_F loss: 0.686573] [D_R loss: 0.680623] [G loss: -0.489988]\n",
      "[Lambda: 0.450000] [Epoch 7400/10000] [D_F loss: 0.686552] [D_R loss: 0.680523] [G loss: -0.489862]\n",
      "[Lambda: 0.450000] [Epoch 7600/10000] [D_F loss: 0.686531] [D_R loss: 0.680453] [G loss: -0.489727]\n",
      "[Lambda: 0.450000] [Epoch 7800/10000] [D_F loss: 0.686515] [D_R loss: 0.680392] [G loss: -0.489591]\n",
      "[Lambda: 0.450000] [Epoch 8000/10000] [D_F loss: 0.686503] [D_R loss: 0.680333] [G loss: -0.489451]\n",
      "[Lambda: 0.450000] [Epoch 8200/10000] [D_F loss: 0.686477] [D_R loss: 0.680280] [G loss: -0.489318]\n",
      "[Lambda: 0.450000] [Epoch 8400/10000] [D_F loss: 0.686446] [D_R loss: 0.680193] [G loss: -0.489181]\n",
      "[Lambda: 0.450000] [Epoch 8600/10000] [D_F loss: 0.686429] [D_R loss: 0.680116] [G loss: -0.489060]\n",
      "[Lambda: 0.450000] [Epoch 8800/10000] [D_F loss: 0.686425] [D_R loss: 0.680100] [G loss: -0.488957]\n",
      "[Lambda: 0.450000] [Epoch 9000/10000] [D_F loss: 0.686403] [D_R loss: 0.680061] [G loss: -0.488874]\n",
      "[Lambda: 0.450000] [Epoch 9200/10000] [D_F loss: 0.686373] [D_R loss: 0.679968] [G loss: -0.488776]\n",
      "[Lambda: 0.450000] [Epoch 9400/10000] [D_F loss: 0.686354] [D_R loss: 0.679901] [G loss: -0.488671]\n",
      "[Lambda: 0.450000] [Epoch 9600/10000] [D_F loss: 0.686357] [D_R loss: 0.679821] [G loss: -0.488576]\n",
      "[Lambda: 0.450000] [Epoch 9800/10000] [D_F loss: 0.686364] [D_R loss: 0.679770] [G loss: -0.488492]\n",
      "Test accuracy: 0.8100000023841858\n",
      "P(y_hat=1 | z=0) = 0.433, P(y_hat=1 | z=1) = 0.550\n",
      "P(y_hat=1 | y=1, z=0) = 0.874, P(y_hat=1 | y=1, z=1) = 0.725\n",
      "Disparate Impact ratio = 0.786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lambda: 0.520000] [Epoch 0/10000] [D_F loss: 0.686341] [D_R loss: 0.716233] [G loss: 0.060358]\n",
      "[Lambda: 0.520000] [Epoch 200/10000] [D_F loss: 0.682480] [D_R loss: 0.705212] [G loss: 0.057829]\n",
      "[Lambda: 0.520000] [Epoch 400/10000] [D_F loss: 0.679609] [D_R loss: 0.698599] [G loss: 0.057678]\n",
      "[Lambda: 0.520000] [Epoch 600/10000] [D_F loss: 0.689413] [D_R loss: 0.702650] [G loss: -0.617674]\n",
      "[Lambda: 0.520000] [Epoch 800/10000] [D_F loss: 0.708826] [D_R loss: 0.711040] [G loss: -0.622047]\n",
      "[Lambda: 0.520000] [Epoch 1000/10000] [D_F loss: 0.713562] [D_R loss: 0.710284] [G loss: -0.617979]\n",
      "[Lambda: 0.520000] [Epoch 1200/10000] [D_F loss: 0.707583] [D_R loss: 0.704548] [G loss: -0.609954]\n",
      "[Lambda: 0.520000] [Epoch 1400/10000] [D_F loss: 0.696940] [D_R loss: 0.697852] [G loss: -0.601033]\n",
      "[Lambda: 0.520000] [Epoch 1600/10000] [D_F loss: 0.687138] [D_R loss: 0.692088] [G loss: -0.593741]\n",
      "[Lambda: 0.520000] [Epoch 1800/10000] [D_F loss: 0.681830] [D_R loss: 0.687905] [G loss: -0.589522]\n",
      "[Lambda: 0.520000] [Epoch 2000/10000] [D_F loss: 0.681902] [D_R loss: 0.685549] [G loss: -0.588647]\n",
      "[Lambda: 0.520000] [Epoch 2200/10000] [D_F loss: 0.685868] [D_R loss: 0.684892] [G loss: -0.589911]\n",
      "[Lambda: 0.520000] [Epoch 2400/10000] [D_F loss: 0.690515] [D_R loss: 0.685109] [G loss: -0.591048]\n",
      "[Lambda: 0.520000] [Epoch 2600/10000] [D_F loss: 0.693174] [D_R loss: 0.685577] [G loss: -0.590852]\n",
      "[Lambda: 0.520000] [Epoch 2800/10000] [D_F loss: 0.693235] [D_R loss: 0.686241] [G loss: -0.589546]\n",
      "[Lambda: 0.520000] [Epoch 3000/10000] [D_F loss: 0.691539] [D_R loss: 0.687218] [G loss: -0.587785]\n",
      "[Lambda: 0.520000] [Epoch 3200/10000] [D_F loss: 0.689151] [D_R loss: 0.688359] [G loss: -0.585881]\n",
      "[Lambda: 0.520000] [Epoch 3400/10000] [D_F loss: 0.686798] [D_R loss: 0.689315] [G loss: -0.583958]\n",
      "[Lambda: 0.520000] [Epoch 3600/10000] [D_F loss: 0.684970] [D_R loss: 0.689726] [G loss: -0.582221]\n",
      "[Lambda: 0.520000] [Epoch 3800/10000] [D_F loss: 0.683956] [D_R loss: 0.689505] [G loss: -0.580886]\n",
      "[Lambda: 0.520000] [Epoch 4000/10000] [D_F loss: 0.683854] [D_R loss: 0.688764] [G loss: -0.580110]\n",
      "[Lambda: 0.520000] [Epoch 4200/10000] [D_F loss: 0.684581] [D_R loss: 0.687718] [G loss: -0.579922]\n",
      "[Lambda: 0.520000] [Epoch 4400/10000] [D_F loss: 0.685862] [D_R loss: 0.686558] [G loss: -0.580204]\n",
      "[Lambda: 0.520000] [Epoch 4600/10000] [D_F loss: 0.687317] [D_R loss: 0.685424] [G loss: -0.580699]\n",
      "[Lambda: 0.520000] [Epoch 4800/10000] [D_F loss: 0.688561] [D_R loss: 0.684392] [G loss: -0.581183]\n",
      "[Lambda: 0.520000] [Epoch 5000/10000] [D_F loss: 0.689290] [D_R loss: 0.683466] [G loss: -0.581463]\n",
      "[Lambda: 0.520000] [Epoch 5200/10000] [D_F loss: 0.689402] [D_R loss: 0.682632] [G loss: -0.581491]\n",
      "[Lambda: 0.520000] [Epoch 5400/10000] [D_F loss: 0.689001] [D_R loss: 0.681877] [G loss: -0.581279]\n",
      "[Lambda: 0.520000] [Epoch 5600/10000] [D_F loss: 0.688337] [D_R loss: 0.681188] [G loss: -0.580946]\n",
      "[Lambda: 0.520000] [Epoch 5800/10000] [D_F loss: 0.687666] [D_R loss: 0.680571] [G loss: -0.580589]\n",
      "[Lambda: 0.520000] [Epoch 6000/10000] [D_F loss: 0.687141] [D_R loss: 0.680036] [G loss: -0.580287]\n",
      "[Lambda: 0.520000] [Epoch 6200/10000] [D_F loss: 0.686797] [D_R loss: 0.679569] [G loss: -0.580035]\n",
      "[Lambda: 0.520000] [Epoch 6400/10000] [D_F loss: 0.686572] [D_R loss: 0.679168] [G loss: -0.579806]\n",
      "[Lambda: 0.520000] [Epoch 6600/10000] [D_F loss: 0.686378] [D_R loss: 0.678841] [G loss: -0.579605]\n",
      "[Lambda: 0.520000] [Epoch 6800/10000] [D_F loss: 0.686155] [D_R loss: 0.678601] [G loss: -0.579445]\n",
      "[Lambda: 0.520000] [Epoch 7000/10000] [D_F loss: 0.685908] [D_R loss: 0.678480] [G loss: -0.579336]\n",
      "[Lambda: 0.520000] [Epoch 7200/10000] [D_F loss: 0.685645] [D_R loss: 0.678423] [G loss: -0.579246]\n",
      "[Lambda: 0.520000] [Epoch 7400/10000] [D_F loss: 0.685409] [D_R loss: 0.678385] [G loss: -0.579179]\n",
      "[Lambda: 0.520000] [Epoch 7600/10000] [D_F loss: 0.685255] [D_R loss: 0.678382] [G loss: -0.579150]\n",
      "[Lambda: 0.520000] [Epoch 7800/10000] [D_F loss: 0.685202] [D_R loss: 0.678353] [G loss: -0.579155]\n",
      "[Lambda: 0.520000] [Epoch 8000/10000] [D_F loss: 0.685283] [D_R loss: 0.678298] [G loss: -0.579197]\n",
      "[Lambda: 0.520000] [Epoch 8200/10000] [D_F loss: 0.685471] [D_R loss: 0.678216] [G loss: -0.579264]\n",
      "[Lambda: 0.520000] [Epoch 8400/10000] [D_F loss: 0.685741] [D_R loss: 0.678145] [G loss: -0.579348]\n",
      "[Lambda: 0.520000] [Epoch 8600/10000] [D_F loss: 0.686051] [D_R loss: 0.678093] [G loss: -0.579417]\n",
      "[Lambda: 0.520000] [Epoch 8800/10000] [D_F loss: 0.686345] [D_R loss: 0.678080] [G loss: -0.579424]\n",
      "[Lambda: 0.520000] [Epoch 9000/10000] [D_F loss: 0.686590] [D_R loss: 0.678108] [G loss: -0.579356]\n",
      "[Lambda: 0.520000] [Epoch 9200/10000] [D_F loss: 0.686762] [D_R loss: 0.678175] [G loss: -0.579197]\n",
      "[Lambda: 0.520000] [Epoch 9400/10000] [D_F loss: 0.686840] [D_R loss: 0.678247] [G loss: -0.578956]\n",
      "[Lambda: 0.520000] [Epoch 9600/10000] [D_F loss: 0.686844] [D_R loss: 0.678297] [G loss: -0.578679]\n",
      "[Lambda: 0.520000] [Epoch 9800/10000] [D_F loss: 0.686796] [D_R loss: 0.678305] [G loss: -0.578403]\n",
      "Test accuracy: 0.8140000104904175\n",
      "P(y_hat=1 | z=0) = 0.513, P(y_hat=1 | z=1) = 0.621\n",
      "P(y_hat=1 | y=1, z=0) = 0.940, P(y_hat=1 | y=1, z=1) = 0.809\n",
      "Disparate Impact ratio = 0.827\n"
     ]
    }
   ],
   "source": [
    "train_result = []\n",
    "train_tensors = Namespace(XS_train = XS_train, y_train = y_train, s1_train = s1_train)\n",
    "val_tensors = Namespace(XS_val = XS_val, y_val = y_val, s1_val = s1_val) \n",
    "test_tensors = Namespace(XS_test = XS_test, y_test = y_test, s1_test = s1_test)\n",
    "\n",
    "train_opt = Namespace(val=len(y_val), n_epochs=10000, k=5, lr_g=0.001, lr_f=0.001, lr_r=0.001)\n",
    "seed = 1\n",
    "\n",
    "lambda_f_set = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.52] # Lambda value for the fairness discriminator of FR-Train.\n",
    "lambda_r = 0.4 # Lambda value for the robustness discriminator of FR-Train.\n",
    "\n",
    "for lambda_f in lambda_f_set:\n",
    "    train_result.append(train_model(train_tensors, val_tensors, test_tensors, train_opt, lambda_f = lambda_f, lambda_r = lambda_r, seed = seed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "------------------ Training Results of FR-Train on poisoned data ------------------\n",
      "[Lambda_f: 0.10] [Lambda_r: 0.40] Accuracy : 0.842, Disparate Impact : 0.657 \n",
      "[Lambda_f: 0.15] [Lambda_r: 0.40] Accuracy : 0.835, Disparate Impact : 0.704 \n",
      "[Lambda_f: 0.20] [Lambda_r: 0.40] Accuracy : 0.833, Disparate Impact : 0.722 \n",
      "[Lambda_f: 0.25] [Lambda_r: 0.40] Accuracy : 0.827, Disparate Impact : 0.745 \n",
      "[Lambda_f: 0.30] [Lambda_r: 0.40] Accuracy : 0.824, Disparate Impact : 0.760 \n",
      "[Lambda_f: 0.35] [Lambda_r: 0.40] Accuracy : 0.821, Disparate Impact : 0.764 \n",
      "[Lambda_f: 0.40] [Lambda_r: 0.40] Accuracy : 0.821, Disparate Impact : 0.770 \n",
      "[Lambda_f: 0.45] [Lambda_r: 0.40] Accuracy : 0.810, Disparate Impact : 0.786 \n",
      "[Lambda_f: 0.52] [Lambda_r: 0.40] Accuracy : 0.814, Disparate Impact : 0.827 \n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------------------------------------------------------\")\n",
    "print(\"------------------ Training Results of FR-Train on poisoned data ------------------\" )\n",
    "for i in range(len(train_result)):\n",
    "    print(\n",
    "        \"[Lambda_f: %.2f] [Lambda_r: %.2f] Accuracy : %.3f, Disparate Impact : %.3f \"\n",
    "        % (train_result[i][0][0], train_result[i][0][1], train_result[i][0][2], train_result[i][0][3])\n",
    "    )       \n",
    "print(\"-----------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
